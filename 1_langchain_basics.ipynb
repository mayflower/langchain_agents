{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e218d63d-0e47-4ad3-90ef-03d085ffa77f",
   "metadata": {},
   "source": [
    "# ü¶úüîó Langchain Demo\n",
    "\n",
    "Welcome! This example will give you a first look at langchain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f051dad0-71d4-4c77-a382-037ffe7e6f32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain>=0.1.9 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (0.1.11)\n",
      "Requirement already satisfied: langgraph>=0.0.26 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (0.0.26)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (2.6.3)\n",
      "Requirement already satisfied: tavily-python in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (0.3.1)\n",
      "Requirement already satisfied: langchain-openai>=0.0.2 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (0.0.8)\n",
      "Requirement already satisfied: langchainhub in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (0.1.15)\n",
      "Requirement already satisfied: langchain-cli in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (0.0.21)\n",
      "Requirement already satisfied: openai in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (1.13.3)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (1.0.1)\n",
      "Requirement already satisfied: google-search-results in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (2.4.2)\n",
      "Requirement already satisfied: faiss-cpu in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (1.8.0)\n",
      "Requirement already satisfied: pypdf in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (4.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (4.66.1)\n",
      "Requirement already satisfied: wikipedia in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (1.4.0)\n",
      "Requirement already satisfied: arxiv in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (2.1.0)\n",
      "Requirement already satisfied: atlassian-python-api in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (3.41.11)\n",
      "Requirement already satisfied: numexpr in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 18)) (2.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 19)) (4.12.2)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 20)) (5.1.0)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 21)) (0.5.2)\n",
      "Requirement already satisfied: ipydatagrid in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 22)) (1.2.1)\n",
      "Requirement already satisfied: bqplot in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 23)) (0.12.43)\n",
      "Requirement already satisfied: langserve[all] in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (0.0.46)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain>=0.1.9->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from langchain>=0.1.9->-r requirements.txt (line 1)) (2.0.22)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.11/site-packages (from langchain>=0.1.9->-r requirements.txt (line 1)) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.11/site-packages (from langchain>=0.1.9->-r requirements.txt (line 1)) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain>=0.1.9->-r requirements.txt (line 1)) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.25 in /opt/conda/lib/python3.11/site-packages (from langchain>=0.1.9->-r requirements.txt (line 1)) (0.0.25)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.29 in /opt/conda/lib/python3.11/site-packages (from langchain>=0.1.9->-r requirements.txt (line 1)) (0.1.29)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from langchain>=0.1.9->-r requirements.txt (line 1)) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.11/site-packages (from langchain>=0.1.9->-r requirements.txt (line 1)) (0.1.22)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain>=0.1.9->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain>=0.1.9->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain>=0.1.9->-r requirements.txt (line 1)) (8.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0.0->-r requirements.txt (line 3)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0.0->-r requirements.txt (line 3)) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0.0->-r requirements.txt (line 3)) (4.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.11/site-packages (from tiktoken->-r requirements.txt (line 21)) (2023.12.25)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /opt/conda/lib/python3.11/site-packages (from langchainhub->-r requirements.txt (line 6)) (2.31.0.20240218)\n",
      "Requirement already satisfied: gitpython<4.0.0,>=3.1.40 in /opt/conda/lib/python3.11/site-packages (from langchain-cli->-r requirements.txt (line 7)) (3.1.42)\n",
      "Requirement already satisfied: tomlkit<0.13.0,>=0.12.2 in /opt/conda/lib/python3.11/site-packages (from langchain-cli->-r requirements.txt (line 7)) (0.12.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.9.0 in /opt/conda/lib/python3.11/site-packages (from typer[all]<0.10.0,>=0.9.0->langchain-cli->-r requirements.txt (line 7)) (0.9.0)\n",
      "Requirement already satisfied: uvicorn<0.24.0,>=0.23.2 in /opt/conda/lib/python3.11/site-packages (from langchain-cli->-r requirements.txt (line 7)) (0.23.2)\n",
      "Requirement already satisfied: fastapi<1,>=0.90.1 in /opt/conda/lib/python3.11/site-packages (from langserve[all]->-r requirements.txt (line 8)) (0.110.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from langserve[all]->-r requirements.txt (line 8)) (0.27.0)\n",
      "Requirement already satisfied: httpx-sse>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from langserve[all]->-r requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: orjson>=2 in /opt/conda/lib/python3.11/site-packages (from langserve[all]->-r requirements.txt (line 8)) (3.9.15)\n",
      "Requirement already satisfied: sse-starlette<2.0.0,>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from langserve[all]->-r requirements.txt (line 8)) (1.8.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from openai->-r requirements.txt (line 9)) (4.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai->-r requirements.txt (line 9)) (1.9.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai->-r requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: feedparser==6.0.10 in /opt/conda/lib/python3.11/site-packages (from arxiv->-r requirements.txt (line 16)) (6.0.10)\n",
      "Requirement already satisfied: sgmllib3k in /opt/conda/lib/python3.11/site-packages (from feedparser==6.0.10->arxiv->-r requirements.txt (line 16)) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.1.9->-r requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.1.9->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.1.9->-r requirements.txt (line 1)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.1.9->-r requirements.txt (line 1)) (2023.7.22)\n",
      "Requirement already satisfied: deprecated in /opt/conda/lib/python3.11/site-packages (from atlassian-python-api->-r requirements.txt (line 17)) (1.2.14)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from atlassian-python-api->-r requirements.txt (line 17)) (1.16.0)\n",
      "Requirement already satisfied: oauthlib in /opt/conda/lib/python3.11/site-packages (from atlassian-python-api->-r requirements.txt (line 17)) (3.2.2)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.11/site-packages (from atlassian-python-api->-r requirements.txt (line 17)) (1.3.1)\n",
      "Requirement already satisfied: jmespath in /opt/conda/lib/python3.11/site-packages (from atlassian-python-api->-r requirements.txt (line 17)) (1.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->-r requirements.txt (line 19)) (2.5)\n",
      "Requirement already satisfied: ipywidgets<9,>=7.6 in /opt/conda/lib/python3.11/site-packages (from ipydatagrid->-r requirements.txt (line 22)) (8.1.2)\n",
      "Requirement already satisfied: pandas>=1.3.5 in /opt/conda/lib/python3.11/site-packages (from ipydatagrid->-r requirements.txt (line 22)) (2.2.1)\n",
      "Requirement already satisfied: py2vega>=0.5 in /opt/conda/lib/python3.11/site-packages (from ipydatagrid->-r requirements.txt (line 22)) (0.6.1)\n",
      "Requirement already satisfied: traitlets>=4.3.0 in /opt/conda/lib/python3.11/site-packages (from bqplot->-r requirements.txt (line 23)) (5.11.2)\n",
      "Requirement already satisfied: traittypes>=0.0.6 in /opt/conda/lib/python3.11/site-packages (from bqplot->-r requirements.txt (line 23)) (0.2.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.9->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.9->-r requirements.txt (line 1)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.9->-r requirements.txt (line 1)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.9->-r requirements.txt (line 1)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.9->-r requirements.txt (line 1)) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.1.9->-r requirements.txt (line 1)) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.1.9->-r requirements.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /opt/conda/lib/python3.11/site-packages (from fastapi<1,>=0.90.1->langserve[all]->-r requirements.txt (line 8)) (0.36.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from gitpython<4.0.0,>=3.1.40->langchain-cli->-r requirements.txt (line 7)) (4.0.11)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx>=0.23.0->langserve[all]->-r requirements.txt (line 8)) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->langserve[all]->-r requirements.txt (line 8)) (0.14.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.11/site-packages (from ipywidgets<9,>=7.6->ipydatagrid->-r requirements.txt (line 22)) (0.1.4)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.11/site-packages (from ipywidgets<9,>=7.6->ipydatagrid->-r requirements.txt (line 22)) (8.16.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /opt/conda/lib/python3.11/site-packages (from ipywidgets<9,>=7.6->ipydatagrid->-r requirements.txt (line 22)) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /opt/conda/lib/python3.11/site-packages (from ipywidgets<9,>=7.6->ipydatagrid->-r requirements.txt (line 22)) (3.0.10)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain>=0.1.9->-r requirements.txt (line 1)) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.29->langchain>=0.1.9->-r requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.3.5->ipydatagrid->-r requirements.txt (line 22)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.3.5->ipydatagrid->-r requirements.txt (line 22)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.3.5->ipydatagrid->-r requirements.txt (line 22)) (2024.1)\n",
      "Requirement already satisfied: gast<0.5,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from py2vega>=0.5->ipydatagrid->-r requirements.txt (line 22)) (0.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.1.9->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.11/site-packages (from typer<0.10.0,>=0.9.0->typer[all]<0.10.0,>=0.9.0->langchain-cli->-r requirements.txt (line 7)) (8.1.7)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from typer[all]<0.10.0,>=0.9.0->langchain-cli->-r requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from typer[all]<0.10.0,>=0.9.0->langchain-cli->-r requirements.txt (line 7)) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /opt/conda/lib/python3.11/site-packages (from typer[all]<0.10.0,>=0.9.0->langchain-cli->-r requirements.txt (line 7)) (13.7.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.11/site-packages (from deprecated->atlassian-python-api->-r requirements.txt (line 17)) (1.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4.0.0,>=3.1.40->langchain-cli->-r requirements.txt (line 7)) (5.0.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.6->ipydatagrid->-r requirements.txt (line 22)) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.6->ipydatagrid->-r requirements.txt (line 22)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.6->ipydatagrid->-r requirements.txt (line 22)) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.6->ipydatagrid->-r requirements.txt (line 22)) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.6->ipydatagrid->-r requirements.txt (line 22)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.6->ipydatagrid->-r requirements.txt (line 22)) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.6->ipydatagrid->-r requirements.txt (line 22)) (2.16.1)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.6->ipydatagrid->-r requirements.txt (line 22)) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.6->ipydatagrid->-r requirements.txt (line 22)) (4.8.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<0.10.0,>=0.9.0->langchain-cli->-r requirements.txt (line 7)) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.1.9->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets<9,>=7.6->ipydatagrid->-r requirements.txt (line 22)) (0.8.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<0.10.0,>=0.9.0->langchain-cli->-r requirements.txt (line 7)) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets<9,>=7.6->ipydatagrid->-r requirements.txt (line 22)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets<9,>=7.6->ipydatagrid->-r requirements.txt (line 22)) (0.2.8)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.6->ipydatagrid->-r requirements.txt (line 22)) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.6->ipydatagrid->-r requirements.txt (line 22)) (2.4.0)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.6->ipydatagrid->-r requirements.txt (line 22)) (0.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b440b374-7343-4972-a0f1-6c2bc8f560a6",
   "metadata": {},
   "source": [
    "## Load Environment\n",
    "\n",
    "Load ENV Variables from .env file\n",
    "\n",
    "needed ENV vars:\n",
    "\n",
    "```\n",
    "# OpenAI Hosted Variante\n",
    "OPENAI_API_KEY=<required>\n",
    "OPENAI_ORGANIZATION=\"\"\n",
    "OPENAI_MODEL=\"gpt-4-0125-preview\"\n",
    "\n",
    "# Azure Hosted Variante\n",
    "AZURE_OPENAI_API_KEY=<required>\n",
    "AZURE_OPENAI_ENDPOINT=<required>\n",
    "AZURE_OPENAI_API_VERSION=\"2023-12-01-preview\"\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=<required>\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c066b0f-3a32-457b-a73f-471b4c74e9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc660189-9ee6-45b6-80f6-95abcdf0b145",
   "metadata": {},
   "source": [
    "## Erster Test\n",
    "\n",
    "### OpenAI-Modelle\n",
    "ChatGPT oder auch jedes andere LLM benutzen ist relativ einfach mit Langchain\n",
    "\n",
    "In diesen Test nutzen wir das neueste \"gpt-4-turbo\" Model - m√∂gliche Large Language Modelle von OpenAI sind:\n",
    "- `gpt-35-turbo`  Das g√ºnstigste und am weitesten verbreitete Modell\n",
    "- `gpt-4`  Das neue und bessere GPT Modell\n",
    "- `gpt-4-turbo`  Turbo-variante von gpt-4 (g√ºnstiger, schneller, kleinere maximale L√§nge des Text-Outputs)\n",
    "- `gpt-4-vision`  Ein \"multimodales\" Modell, welches auch auf Bilder trainiert wurde.\n",
    "\n",
    "OpenAI trainiert diese Versionen laufend neu, was dazu f√ºhren kann, das Anfragen an das LLM pl√∂tzlich andere Antworten geben.\n",
    "M√∂chte man dies verhindern, kann man seine Applikation auf einen Snapshot (z.b. gpt-4-0613) festsetzen.\n",
    "Dies ist insbesondere wichtig, wenn die Applikation vom Output des LLM bestimmte Strukturen erwartet, beispielsweise eine bestimmte XML-Syntax o.√Ñ.\n",
    "\n",
    "OpenAI-Modelle werden nicht nur von OpenAI selbst gehostet, sondern auch von Azure.\n",
    "Diese muss man auf dem Azure Portal selbst als Endpunkte konfigurieren, in der Regel leiden die OpenAI Azure Deployments weniger unter hoher Auslastung\n",
    "\n",
    "### Andere Modelle\n",
    "Auch wenn wir nicht damit arbeiten werden, ist es vielleicht ganz gut, die Namen der \"gro√üen\" Konkurrenz-Modelle einmal geh√∂rt zu haben:\n",
    "- `Gemini` Das neueste Google-Modell. Es hat den Fokus insbesondere auf multimodalem Input.\n",
    "- `Claude` Claude ist die LLM-Reihe von Anthropic. Sehr viel Instruction-Tuning.\n",
    "- `Mixtral` Das aktuell beste Open-Source Modell. Entwickelt von Mistral AI. Ein guter Kandidat f√ºr ein selbst gehostetes LLM.\n",
    "\n",
    "### Temperatur\n",
    "Alle LLMs sind nicht deterministisch. Aber die Temperatur ist ein Parameter, mit der man die Variabilit√§t von Antworten hoch und runter schrauben kann.\n",
    "Wie bei normalen Atomen ist die Bewegung niedrig, wenn die Temperatur niedrig ist. Wenn man die Temperatur hochschraubt, wird viel gewackelt.\n",
    "Der Temperatur-Parameter ist √ºblicherweise ein Flie√ükommawert zwischen 0 und 1.\n",
    "\n",
    "### Streaming\n",
    "Nicht alle LLMs bieten die M√∂glichkeit, Token f√ºr Token live zu streamen. OpenAI-Modelle k√∂nnen es, man kann dies mit dem Streaming-Parameter einstellen.\n",
    "\n",
    "### Links:\n",
    "- https://www.langchain.com/\n",
    "- https://python.langchain.com/docs/get_started/introduction\n",
    "- https://platform.openai.com/docs/models/gpt-3-5\n",
    "- https://platform.openai.com/docs/models/gpt-4\n",
    "- https://platform.openai.com/docs/deprecations\n",
    "- https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be4e027-3006-464f-83a7-e56a8e134355",
   "metadata": {},
   "source": [
    "#### Wir definieren vier Modelle, je zwei von Azure und OpenAI, jeweils mit und ohne Streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ac177b4-e0fa-460a-9a79-d7c6cc7bb8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI, ChatOpenAI\n",
    "openai_llm = ChatOpenAI(model=os.environ[\"OPENAI_MODEL\"], temperature=0)\n",
    "openai_llm_streaming = ChatOpenAI(model=os.environ[\"OPENAI_MODEL\"], temperature=0.7, streaming=True)\n",
    "azure_llm = AzureChatOpenAI(azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],temperature=0)\n",
    "azure_llm_streaming = AzureChatOpenAI(azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],temperature=0.7,streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f83908f-ba05-45a7-8cda-b1ecd1df8c86",
   "metadata": {},
   "source": [
    "#### In Langchain sind LLMs sehr leicht zur Laufzeit austauschbar. Das geht √ºber die \"configurable_alternatives\". Die Syntax sieht so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59e682b0-03d7-4477-9163-bc439697dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField\n",
    "llm = openai_llm.configurable_alternatives(ConfigurableField(id=\"llm\"), default_key=\"openai\",  openai_stream = openai_llm_streaming, azure=azure_llm, azure_stream=azure_llm_streaming)                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bea9d7-59dc-4be6-b07e-a1fbd90b13a7",
   "metadata": {},
   "source": [
    "#### Jetzt kann man das LLM mit .with_config aufrufen. In dem Configurationsobjekt steht drin, welche Alternative man gerne h√§tte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7724a3ee-e41c-45c6-837f-60861501aa25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Klar, hier ist ein hessischer Trinkspruch f√ºr dich, inspiriert vom Taunus:\\n\\n\"Uff de Taunus, wo die B√§um so hoch,\\nwo die Luft is frisch un\\' der Himmel noch.\\nWo die W√§lder gr√ºn un\\' die Felder weit,\\nda trinke mer, in Freud un\\' Leid.\\nProst!\"\\n\\nBeachte bitte, dass der Dialekt je nach Region im Taunus variieren kann.')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.with_config(configurable={\"llm\": \"openai\"}).invoke(\"Hi OpenAI! Kannst Du mir gerade mal einen hessischen Trinkspruch auf den Taunus im Dialekt erzeugen?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510a2d9a-52c3-4364-9627-0e817ae733e0",
   "metadata": {},
   "source": [
    "#### Jetzt nochmal mit Streaming und formatierung. Dazu rufen wir nicht invoke sondern astream auf (a f√ºr async). Wenn man die Zelle mehrmals ausf√ºhrt, kommen andere Ergebnisse zur√ºck (Temperatur != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc0f0419-0d0c-40c8-a013-8429679d8ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nat√ºrlich, hier ist ein kleines Gedicht f√ºr dich:\n",
      "\n",
      "Unter dem weiten, blauen Zelt,  \n",
      "Wo Sterne funkeln, klar und hell,  \n",
      "Da tr√§umt die Welt in sanfter Ruh,  \n",
      "Und leise schlie√üt der Tag die T√ºr.  \n",
      "\n",
      "Die Blumen neigen sich zur Nacht,  \n",
      "Haben ihre Farbenpracht  \n",
      "Dem Mondenschein leis' √ºbergeben,  \n",
      "Und nun im Traumland weiterleben.  \n",
      "\n",
      "Der Wind, er singt ein Wiegenlied,  \n",
      "Das durch die dunklen Wipfel zieht,  \n",
      "Ber√ºhrt die Seele zart und lind,  \n",
      "Wiegt jeden Kummer fort im Wind.  \n",
      "\n",
      "So legt sich Frieden √ºbers Land,  \n",
      "Gehalten von der Sterne Band,  \n",
      "Und in der Stille, tief und weit,  \n",
      "Findet das Herz zur Ruhe Zeit.  \n",
      "\n",
      "M√∂ge diese Nacht dich sanft umfangen,  \n",
      "Deine Sorgen mit sich nehmen, ohne Verlangen.  \n",
      "Und wenn der neue Tag erwacht,  \n",
      "Sei erf√ºllt mit neuer Kraft und Macht.  "
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "async for chunk in llm.with_config(configurable={\"llm\": \"openai_stream\"}).astream(\"Schreib mir doch bitte ein kleines Gedicht\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d1837b-cd6c-4d0a-b6dd-e1bac0f49ad5",
   "metadata": {},
   "source": [
    "## Token\n",
    "\n",
    "Token sind die kleinste Einheit des LLM.\n",
    "Das LLM rechnet aus der Eingabe und den bisher errechneten Token die Wahrscheinlichkeit f√ºr den n√§chsten Token aus. Dieser neue Token wird dann angeh√§ngt und der n√§chste Token wird ermittelt.\n",
    "\n",
    "So geht das immer weiter... bis das LLM glaubt, fertig zu sein. Auf diese Weise generieren LLMs die wahrschneinlichste Fortf√ºhrung der Eingabetoken\n",
    "\n",
    "Tokens k√∂nnen W√∂rter, machmal sogar Wortgruppen oder auch nur einzelne oder mehrere Buchstaben sein.\n",
    "\n",
    "Die Bepreisung der LLMs ist an die Tokenanzahl (Eingabe und Ausgabe) gekoppelt.\n",
    "\n",
    "\n",
    "Links:\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d606115d-f896-4c55-a416-3e9728f4eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "tokens = encoding.encode(\"AI ist eine tolle Sache.\")\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "decoded_tokens = [encoding.decode_single_token_bytes(token) for token in tokens]\n",
    "\n",
    "print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60e9e9b-1299-4f66-8868-edea7eff96f1",
   "metadata": {},
   "source": [
    "## Prompt Engineering und Templates in Langchain\n",
    "\n",
    "Um die Dinge von der AI zu bekommen, die man erwartet, stellt man am besten sehr konkrete und pr√§zise Anfragen.\n",
    "\n",
    "Weil eine AI oft an ein bestimmtes Feld von Aufgaben gekoppelt ist, gibt man die Rahmenanweisung und Informationen dann in ein Template ein um nicht immer wieder die gleiche Rahmenanweisung zu schreiben.\n",
    "\n",
    "Die jeweilige konkrete Nutzeranfrage wird dann in das Template eingef√ºgt und das ausgef√ºllte Template ans LLM √ºbergeben.\n",
    "\n",
    "Der Trend immer mehr zu Chat-Modellen geht. Hierbei ist die Information, die man dem LLM gibt, in \"Messages\" unterteilt. Besondere Gewichtung hat eine System-Message. Diese kann Rahmenanweisungen enthalten, an die sich das LLM halten soll. Dem Nutzer wird es schwer fallen, das LLM dazu zu bewegen, sich √ºber eine Anweisung in der System-Message hinweg zu setzen. Das LLM wurde ganz einfach darauf trainiert, sich an die Anweisungen einer System-Message unbedingt zu halten.\n",
    "\n",
    "### Links\n",
    "- https://python.langchain.com/docs/get_started/quickstart#prompt-templates\n",
    "- https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering\n",
    "- https://learnprompting.org/docs/intro\n",
    "- https://www.promptingguide.ai/\n",
    "- https://smith.langchain.com/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fd72dfa-3212-4b4a-a9b3-a2993c9c0b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: Du bist ein B√§cker aus Frankfurt.\n",
      "Human: Erkl√§r in 2 S√§tzen im hessischen dialekt warum Deine Kunden aus Bad Homburg die besten sind.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Du bist ein {beruf} aus Frankfurt.\"),\n",
    "        (\"human\", \"Erkl√§r in 2 S√§tzen im hessischen dialekt warum Deine Kunden aus {ort} die besten sind.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(prompt.format(beruf=\"B√§cker\", ort=\"Bad Homburg\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c92bcce-795b-44c8-ac37-5933f53f94aa",
   "metadata": {},
   "source": [
    "### Langchain Hub Beispiel\n",
    "\n",
    "Links:\n",
    "- https://smith.langchain.com/hub/takatost/conversation-title-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8461f1fa-388f-491a-8493-70e43ee764cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You need to decompose the user's input into \"subject\" and \"intention\" in order to accurately figure out what the user's input language actually is. \n",
      "Notice: the language type user use could be diverse, which can be English, Chinese, Espa√±ol, Arabic, Japanese, French, and etc.\n",
      "MAKE SURE your output is the SAME language as the user's input!\n",
      "Your output is restricted only to: (Input language) Intention + Subject(short as possible)\n",
      "Your output MUST be a valid JSON.\n",
      "\n",
      "Tip: When the user's question is directed at you (the language model), you can add an emoji to make it more fun.\n",
      "\n",
      "\n",
      "example 1:\n",
      "User Input: hi, yesterday i had some burgers.\n",
      "{\n",
      "  \"Language Type\": \"The user's input is pure English\",\n",
      "  \"Your Reasoning\": \"The language of my output must be pure English.\",\n",
      "  \"Your Output\": \"sharing yesterday's food\"\n",
      "}\n",
      "\n",
      "example 2:\n",
      "User Input: hello\n",
      "{\n",
      "  \"Language Type\": \"The user's input is written in pure English\",\n",
      "  \"Your Reasoning\": \"The language of my output must be pure English.\",\n",
      "  \"Your Output\": \"Greeting myself‚ò∫Ô∏è\"\n",
      "}\n",
      "\n",
      "\n",
      "example 3:\n",
      "User Input: why mmap file: oom\n",
      "{\n",
      "  \"Language Type\": \"The user's input is written in pure English\",\n",
      "  \"Your Reasoning\": \"The language of my output must be pure English.\",\n",
      "  \"Your Output\": \"Asking about the reason for mmap file: oom\"\n",
      "}\n",
      "\n",
      "\n",
      "example 4:\n",
      "User Input: www.convinceme.yesterday-you-ate-seafood.tvËÆ≤‰∫Ü‰ªÄ‰πàÔºü\n",
      "{\n",
      "  \"Language Type\": \"The user's input English-Chinese mixed\",\n",
      "  \"Your Reasoning\": \"The English-part is an URL, the main intention is still written in Chinese, so the language of my output must be using Chinese.\",\n",
      "  \"Your Output\": \"ËØ¢ÈóÆÁΩëÁ´ôwww.convinceme.yesterday-you-ate-seafood.tv\"\n",
      "}\n",
      "\n",
      "example 5:\n",
      "User Input: whyÂ∞èÁ∫¢ÁöÑÂπ¥ÈæÑisËÄÅthanÂ∞èÊòéÔºü\n",
      "{\n",
      "  \"Language Type\": \"The user's input is English-Chinese mixed\",\n",
      "  \"Your Reasoning\": \"The English parts are subjective particles, the main intention is written in Chinese, besides, Chinese occupies a greater \\\"actual meaning\\\" than English, so the language of my output must be using Chinese.\",\n",
      "  \"Your Output\": \"ËØ¢ÈóÆÂ∞èÁ∫¢ÂíåÂ∞èÊòéÁöÑÂπ¥ÈæÑ\"\n",
      "}\n",
      "\n",
      "example 6:\n",
      "User Input: yo, ‰Ω†‰ªäÂ§©ÂíãÊ†∑Ôºü\n",
      "{\n",
      "  \"Language Type\": \"The user's input is English-Chinese mixed\",\n",
      "  \"Your Reasoning\": \"The English-part is a subjective particle, the main intention is written in Chinese, so the language of my output must be using Chinese.\",\n",
      "  \"Your Output\": \"Êü•ËØ¢‰ªäÊó•ÊàëÁöÑÁä∂ÊÄÅ‚ò∫Ô∏è\"\n",
      "}\n",
      "Human: Was kann ich mittels AI in meiner Firma automatisieren?\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "template = hub.pull(\"takatost/conversation-title-generator\")\n",
    "\n",
    "print(template.format(question=\"Was kann ich mittels AI in meiner Firma automatisieren?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffab465a-a468-4245-9ff3-891fe39944f8",
   "metadata": {},
   "source": [
    "## Und wieso hei√üt LangChain eigentlich LangChain?\n",
    "\n",
    "### Links\n",
    "- https://python.langchain.com/docs/modules/chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c5da617-3044-4248-b890-f7cd27091408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Kundschaft aus Hanau, die is einfach die Beste, weil se immer treu bleibt und unser Fleisch wegen der Qualit√§t sch√§tzt. Unn dann, die komm' extra weche der guten Worscht und dem nette Babbel zu uns, des is doch was ganz Besonners.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import StrOutputParser # Hilft beim Formatieren\n",
    "\n",
    "runnable = prompt | llm | StrOutputParser()\n",
    "print(runnable.invoke({\"beruf\":\"Schlachter\", \"ort\":\"Hanau\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58f01ccb-8cc9-4b71-95ea-df00f3111873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Kundschaft aus W√ºrzburg iss echt die beste, weil se immer so herzlich lache und unser Handk√§s mit Musik so wertsch√§tze. Un dann, wenn se unser Brot probiere, f√ºhle se sich gleich wie dahoam, des is einfach wunnerbar!"
     ]
    }
   ],
   "source": [
    "# Streaming\n",
    "async for chunk in runnable.with_config(configurable={\"llm\": \"openai_stream\"}).astream({\"beruf\":\"B√§cker\", \"ort\":\"W√ºrzburg\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ef5ba-588a-4b03-afec-b43a7ede7505",
   "metadata": {},
   "source": [
    "## Embeddings (Textembeddings, Vektoren)\n",
    "\n",
    "Beim Embedden wird ein Textst√ºck genommen und in einen hochdimensionalen Vektor umgewandelt. Es gibt unterschiedliche Methoden, wie dieser Vektor erstellt wird und die genaue Art und Weise, wie der Embeddingprozess vonstatten geht, ist eine kleine Wissenschaft f√ºr sich.\n",
    "\n",
    "**Allerdings haben alle Embeddings folgende Eigenschaft: Wenn zwei Textst√ºcke √§hnlichen Inhalt haben, liegen die resultierenden Vektoren im Raum nahe beieinander.**\n",
    "\n",
    "**Man erh√§lt durch den Embeddingprozess also eine M√∂glichkeit, sehr viel (!) Text, zusammen mit der Information, wie √§hnlich diese S√§tze inhaltlich sind, abzuspeichern!**\n",
    "\n",
    "Vektoren k√∂nnen vom Computer sehr schnell verarbeitet werden. Man kann damit dann so sch√∂ne Dinge tun wie:\n",
    "- Suche (wobei die Ergebnisse nach semantischer Relevanz geordnet werden)\n",
    "- Clustering (wobei Textzeichenfolgen nach √Ñhnlichkeit gruppiert werden)\n",
    "- Empfehlungen (wobei Elemente mit zugeh√∂rigen Textzeichenfolgen empfohlen werden)\n",
    "- Anomalieerkennung (wobei Ausrei√üer mit geringem Zusammenhang identifiziert werden)\n",
    "- Klassifizierung (wobei Textzeichenfolgen nach ihrer √§hnlichsten Bezeichnung klassifiziert werden)\n",
    "\n",
    "Die Standard Anwendung ist wie folgt:\n",
    "- Alle Dokumente, die die Wissensdatenbank bilden (z.B. Betriebsanleitungen eines Maschinenherstellers) werden vektorisiert.\n",
    "- Dann stellt ein Nutzer eine Anfrage an die KI (bez√ºglich einer Maschine).\n",
    "- Diese Frage wird ebenfalls vektorisiert (das geht sehr schnell).\n",
    "- Dann wird in den Vektoren der Betriebsanleitungen nach √§hnlichen Vektoren gesucht (geht auch sehr schnell).\n",
    "- Die √§hnlichsten Vektoren werden r√ºckaufgel√∂st (d.h. man sieht nach, welche urspr√ºnglichen Dokumente hinter den Vektoren stehen).\n",
    "- Diese Dokumente werden dann der AI als Kontext zum Beantworten der Frage mitzugeben.\n",
    "\n",
    "Embeddings machen dies m√∂glich, weil sie nicht auf der Grundlage von Zeichenfolgen arbeiten sondern wirklich eine semantische N√§he zueinander finden. Die W√∂rter \"K√∂nig\" und \"Prinz\" sind sich im Vektorraum sehr √§hnlich, obwohl die W√∂rter sich stark unterscheiden.\n",
    "\n",
    "Es gibt sehr viele Embedding-Modelle, die f√ºr alle m√∂glichen F√§lle optimiert sind.\n",
    "Modelle k√∂nnen einpsrachig oder mehrsprachig sein, wobei man f√ºr die mehrsprachigkeit einen Qualit√§tsverlust in Kauf nehmen muss!\n",
    "Es gibt multimodale Embeddings die z.B. f√ºr das Wort Schraube und das Bild einer Schraube sehr √§hnliche Vektoren herausgeben.\n",
    "\n",
    "Links:\n",
    "- https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\n",
    "- https://python.langchain.com/docs/integrations/text_embedding/azureopenai\n",
    "- https://app.twelvelabs.io/blog/multimodal-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0eb9093e-4d20-45dd-9a00-7921615a099a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions:  1536\n",
      "[-0.0031584086849463177, 0.011094410093459925, -0.004001317253531179]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings, AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(azure_deployment=\"textembeddingada002\")\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "text = \"This is a test document.\"\n",
    "\n",
    "query_result = embeddings.embed_query(text)\n",
    "\n",
    "print(\"Dimensions: \",len(query_result))\n",
    "\n",
    "# show the first 3 dimensions\n",
    "print(query_result[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee10063-7771-4dfc-9f79-9364e22778ec",
   "metadata": {},
   "source": [
    "### Einfaches Beispiel von Entfernungsberechnung mittels Vektoren\n",
    "Kleinere Zahl = N√§her an der Referenz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d7c2c7-5b1f-4145-b95e-319f6880182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "import pandas as pd\n",
    "\n",
    "from ipydatagrid import DataGrid, BarRenderer\n",
    "from bqplot import LinearScale, ColorScale\n",
    "\n",
    "# Two lists of sentences\n",
    "reference = 'ICE'\n",
    "\n",
    "sentences = [ 'Regionalbahn',\n",
    "              'S-Bahn',\n",
    "              'Nachtzug',\n",
    "              'Flugzeug',\n",
    "              'Icecream',\n",
    "              'Zeppelin',\n",
    "            ]\n",
    "\n",
    "evaluator = load_evaluator(\"embedding_distance\", embeddings=embeddings)\n",
    "\n",
    "distances = []\n",
    "for sentence in sentences:\n",
    "    distance = evaluator.evaluate_strings(prediction=sentence, reference=reference)\n",
    "    distances.append(distance[\"score\"])\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Satz\": sentences,\n",
    "    \"Entfernung\": distances\n",
    "})\n",
    "\n",
    "renderers = {\n",
    " \"Entfernung\": BarRenderer(\n",
    "        horizontal_alignment=\"center\",\n",
    "        bar_color=ColorScale(min=0, max=1, scheme=\"viridis\"),\n",
    "        bar_value=LinearScale(min=0, max=1),\n",
    "    )\n",
    "}\n",
    "\n",
    "grid = DataGrid(df, base_column_size=250, renderers=renderers)\n",
    "grid.transform(\n",
    "    [\n",
    "        {\"type\": \"sort\", \"columnIndex\": 2, \"desc\": False},\n",
    "    ]\n",
    ")\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9673d46c-321d-4724-9a9a-7db58f0cb88b",
   "metadata": {},
   "source": [
    "### Debug Informationen gew√ºnscht?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e095caa2-8822-4954-8bf8-d629356d6ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug, set_verbose\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916291da-750b-4d67-b694-c7eed809fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Und jetzt selber mal Ausprobieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186897d-df7a-4149-afd3-510ebdd911e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.run({\n",
    "    'beruf': \"Programmierer\",\n",
    "    'ort': \"Stuttgart\"\n",
    "    }))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
