{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## ðŸ¦œðŸ”— LangChain Agent mit einem SQL Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! [ ! -f northwind.db ] && bunzip2 northwind.db.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "#### Wir erstellen das SQL Toolkit und den Agenten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_agent_executor\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain import hub\n",
    "from helpers import llm\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "db = SQLDatabase.from_uri(\"sqlite:///northwind.db\")\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm())\n",
    "tools = toolkit.get_tools()\n",
    "prompt: ChatPromptTemplate = hub.pull(\"reactagent/sql\")\n",
    "prompt = prompt.partial(dialect=toolkit.dialect, top_k=10)\n",
    "agent_runnable = create_openai_functions_agent(llm(), tools, prompt)\n",
    "agent_excutor = create_agent_executor(agent_runnable, tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "#### Wir definieren eine Funktion, die die gestreamte Ausgabe des Agenten formatiert.\n",
    "\n",
    "Hinweis: Dies hier ist noch kein \"echtes\" Agenten-Streaming. Die gestreamten Chunks sind die kompletten Zwischenschritte des Agenten und der Tools, keine Token.\n",
    "\n",
    "Man muss den folgenden Code nicht durchlesen. Man sollte sich nur merken, dass man solche Code-Brocken Ã¼blicherweise selbst erstellen muss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Any, AsyncIterator, List, Tuple\n",
    "from langchain_core.agents import AgentActionMessageLog, AgentFinish\n",
    "\n",
    "async def formatted_output_streamer(stream: AsyncIterator[Any]) -> AsyncIterator[Any]:\n",
    "    async for chunk in stream:\n",
    "        output = \"\"\n",
    "        for key, value in chunk.items():\n",
    "            if key == \"agent\":\n",
    "                outcome  = value.get(\"agent_outcome\")\n",
    "                if isinstance(outcome, AgentActionMessageLog):\n",
    "                    output += f\"Agent log:\\n\\n{outcome.log.strip()}\"\n",
    "                elif isinstance(outcome, AgentFinish):\n",
    "                    output += f\"Agent finished:\\n\\n{outcome.log.strip()}\"\n",
    "                output +=\"\\n\\n----------------------------------------------------------------------------------------\\n\\n\"\n",
    "            elif key == \"action\":\n",
    "                steps: List[Tuple[AgentActionMessageLog, str]] = value.get(\"intermediate_steps\")\n",
    "                for index, step in enumerate(steps):\n",
    "                    output += f\"Tool log:\\n\\n{step[1].strip()}\"\n",
    "                    if index < len(steps) - 1:\n",
    "                        print(\"----------------\")\n",
    "                output +=\"\\n\\n----------------------------------------------------------------------------------------\\n\\n\"\n",
    "            elif key == \"__end__\":\n",
    "                output =\"Done\"\n",
    "        yield output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### Wir pipen (chainen) den Agenten mit dem Formatierer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = agent_excutor | formatted_output_streamer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Looos...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"input\": \"Where do i find the orders?\"}\n",
    "async for chunk in app.astream(inputs):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"input\": \"Are there any order from Munich?\"}\n",
    "async for chunk in app.astream(inputs):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"input\": \"Which Employee has the most orders?\"}\n",
    "async for chunk in app.astream(inputs):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### Schauen wir mal, ob er das hier hinbekommt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"input\": \"Which Customer has had the Order with the highest total cost ever? What was the Order Id?\"}\n",
    "async for chunk in app.astream(inputs):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### Und noch einmal das batchen demonstrieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def formatted_output_batcher(item: Any)-> str:\n",
    "    return [item.get(\"input\"), item.get(\"agent_outcome\").return_values.get(\"output\")]\n",
    "\n",
    "batcher = agent_excutor | formatted_output_batcher\n",
    "\n",
    "result = await batcher.abatch([\n",
    "    {\"input\": \"Where do i find the orders?\"},\n",
    "    {\"input\": \"Are there any order from Munich?\"},\n",
    "    {\"input\": \"Which Employee has the most orders?\"},\n",
    "    {\"input\": \"Which Customer has had the Order with the highest total cost ever? What was the Order Id?\"}\n",
    "])\n",
    "\n",
    "for index, item in enumerate(result):\n",
    "    print(f\"Query {index+1}:\")\n",
    "    print(f\"Question: {item[0]}\")\n",
    "    print(f\"Answer: {item[1]}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
