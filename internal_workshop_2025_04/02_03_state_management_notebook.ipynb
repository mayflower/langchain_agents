{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Management in LLM-Anwendungen\n",
    "\n",
    "In diesem Notebook lernen wir, wie wir Zustände (State) in KI-Anwendungen verwalten können. Dies ist besonders wichtig für Konversationen und komplexe Workflows mit Large Language Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Warum ist State Management wichtig?\n",
    "\n",
    "LLMs sind grundsätzlich zustandslos - sie haben keine inhärente Fähigkeit, sich an vorherige Interaktionen zu erinnern. Jede Anfrage wird isoliert betrachtet.\n",
    "\n",
    "**Herausforderungen ohne State Management:**\n",
    "- Keine Kontexterhaltung zwischen Anfragen\n",
    "- Unmöglichkeit, auf vorherige Informationen zu referenzieren\n",
    "- Keine Möglichkeit für mehrstufige Interaktionen\n",
    "\n",
    "**Vorteile mit State Management:**\n",
    "- Natürliche Konversationen durch Kontexterhaltung\n",
    "- Effizienzsteigerung durch Vermeiden von Wiederholungen\n",
    "- Möglichkeit für komplexe, mehrstufige Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benötigte Bibliotheken installieren\n",
    "!pip install -q langchain langchain-openai langchain-community dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Lade Umgebungsvariablen aus der .env Datei\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'OPENAI_API_KEY'\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM initialisieren\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Speichertypen in LangChain\n",
    "\n",
    "LangChain bietet verschiedene Memory-Typen an, die für unterschiedliche Anwendungsfälle optimiert sind:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 ConversationBufferMemory\n",
    "\n",
    "Dies ist der einfachste Speichertyp - speichert den gesamten Konversationsverlauf als Liste von Nachrichten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Erstellen des Speichers\n",
    "buffer_memory = ConversationBufferMemory()\n",
    "\n",
    "# Speichern von Kontext\n",
    "buffer_memory.save_context({\"input\": \"Hallo, ich bin Anna\"}, {\"output\": \"Hallo Anna! Wie kann ich dir helfen?\"})\n",
    "buffer_memory.save_context({\"input\": \"Ich interessiere mich für maschinelles Lernen.\"},\n",
    "                           {\"output\": \"Das ist ein spannendes Thema! Möchtest du mehr darüber erfahren?\"})\n",
    "\n",
    "# Laden des gespeicherten Kontexts\n",
    "print(buffer_memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration in eine Konversation\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=buffer_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = conversation.predict(input=\"Wie heiße ich?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vorteile:**\n",
    "- Einfach zu implementieren\n",
    "- Vollständiger Konversationsverlauf verfügbar\n",
    "\n",
    "**Nachteile:**\n",
    "- Speicherbedarf wächst mit der Konversationslänge\n",
    "- Probleme mit dem Kontextfenster des LLM bei langen Gesprächen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ConversationSummaryMemory\n",
    "\n",
    "Dieses Memory fasst den Konversationsverlauf dynamisch zusammen, um den Speicherbedarf zu reduzieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# Erstellen eines Summary Memory\n",
    "summary_memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "# Speichern von Kontext\n",
    "summary_memory.save_context({\"input\": \"Hallo, ich bin Michael und arbeite als Softwareentwickler.\"},\n",
    "                            {\"output\": \"Hallo Michael! Schön, einen Softwareentwickler kennenzulernen.\"})\n",
    "summary_memory.save_context({\"input\": \"Ich möchte eine KI-Anwendung für mein Unternehmen entwickeln.\"},\n",
    "                            {\"output\": \"Das klingt spannend! Welche Art von KI-Anwendung schwebt dir vor?\"})\n",
    "summary_memory.save_context({\"input\": \"Eine, die Kundenfeedback automatisch analysieren kann.\"},\n",
    "                            {\"output\": \"Sentiment-Analyse ist ein guter Ansatz für die Analyse von Kundenfeedback.\"})\n",
    "\n",
    "# Zusammenfassung anzeigen\n",
    "print(summary_memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration in eine Konversation\n",
    "summary_conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=summary_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = summary_conversation.predict(input=\"Kannst du mir mehr über Sentiment-Analyse erzählen?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vorteile:**\n",
    "- Effiziente Nutzung des Kontextfensters\n",
    "- Gut für längere Konversationen\n",
    "\n",
    "**Nachteile:**\n",
    "- Kann Details verlieren\n",
    "- Benötigt zusätzliche LLM-Aufrufe für die Zusammenfassung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 VectorStoreMemory\n",
    "\n",
    "Dieser Speichertyp nutzt Vektorähnlichkeiten, um relevante Teile früherer Konversationen abzurufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import faiss\n",
    "\n",
    "# Embeddings erstellen\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docstore = InMemoryDocstore({})\n",
    "\n",
    "# Vector Store erstellen\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=faiss.IndexFlatL2(1536),  # Dimensionalität der OpenAI-Embeddings\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "# Retriever erstellen\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Vector Memory initialisieren\n",
    "vector_memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "\n",
    "# Speichern von Kontext\n",
    "vector_memory.save_context(\n",
    "    {\"input\": \"Mein Name ist Julia und ich bin Datenanalystin.\"},\n",
    "    {\"output\": \"Hallo Julia! Schön, dass du dich mit Datenanalyse beschäftigst.\"}\n",
    ")\n",
    "vector_memory.save_context(\n",
    "    {\"input\": \"Ich arbeite mit Python und nutze hauptsächlich Pandas und scikit-learn.\"},\n",
    "    {\"output\": \"Das sind hervorragende Tools für die Datenanalyse und maschinelles Lernen.\"}\n",
    ")\n",
    "vector_memory.save_context(\n",
    "    {\"input\": \"Ich möchte meine Fähigkeiten im Bereich Deep Learning verbessern.\"},\n",
    "    {\"output\": \"Für Deep Learning empfehle ich dir, TensorFlow oder PyTorch zu lernen.\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abfrage des relevanten Kontexts\n",
    "print(vector_memory.load_memory_variables({\"input\": \"Mit welchen Tools arbeite ich?\"})[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vorteile:**\n",
    "- Semantische Suche nach relevanten Informationen\n",
    "- Nicht linear abhängig von der Konversationslänge\n",
    "\n",
    "**Nachteile:**\n",
    "- Komplexere Implementierung\n",
    "- Benötigt Embedding-Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Kombination verschiedener Memory-Typen\n",
    "\n",
    "Für komplexere Anwendungen kann man verschiedene Memory-Typen kombinieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import CombinedMemory\n",
    "\n",
    "# Zwei verschiedene Speichertypen erstellen\n",
    "conv_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "summary_memory_combined = ConversationSummaryMemory(llm=llm, memory_key=\"summary\")\n",
    "\n",
    "# Kombination der Speicher\n",
    "combined_memory = CombinedMemory(memories=[conv_memory, summary_memory_combined])\n",
    "\n",
    "# Speichern von Kontext\n",
    "combined_memory.save_context(\n",
    "    {\"input\": \"Hallo, ich bin Thomas und interessiere mich für KI.\"},\n",
    "    {\"output\": \"Hallo Thomas! KI ist ein faszinierendes Thema.\"}\n",
    ")\n",
    "combined_memory.save_context(\n",
    "    {\"input\": \"Besonders interessiert mich der Bereich Natural Language Processing.\"},\n",
    "    {\"output\": \"NLP ist ein zentraler Bereich der KI mit vielen praktischen Anwendungen.\"}\n",
    ")\n",
    "\n",
    "# Gespeicherte Variablen anzeigen\n",
    "print(combined_memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. State Management in LangGraph\n",
    "\n",
    "LangGraph erweitert die Möglichkeiten von LangChain mit zustandsbasierter Verarbeitung und bietet typisierte Zustände für komplexe Workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict, List, Literal\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "import operator\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "# Definition eines typisierten Zustands\n",
    "class ConversationState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]  # Liste von Nachrichten, die durch Operator '+' zusammengeführt werden\n",
    "    next_step: str  # Kontrolle des Workflow-Flusses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node-Funktionen definieren\n",
    "def chat_node(state: ConversationState) -> ConversationState:\n",
    "    \"\"\"LLM-Node, der auf Benutzereingaben reagiert\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    # Überprüfen, ob eine Datenbank-Abfrage notwendig ist\n",
    "    if \"Datenbank\" in messages[-1].content or \"Suche\" in messages[-1].content:\n",
    "        return {\"messages\": messages + [response], \"next_step\": \"database\"}\n",
    "    else:\n",
    "        return {\"messages\": messages + [response], \"next_step\": \"end\"}\n",
    "\n",
    "\n",
    "def database_query(state: ConversationState) -> ConversationState:\n",
    "    \"\"\"Simuliert eine Datenbankabfrage\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # Einfache Simulation einer Datenbankabfrage\n",
    "    system_message = AIMessage(content=\"Ich habe in der Datenbank folgende Informationen gefunden: ...\")\n",
    "\n",
    "    return {\"messages\": messages + [system_message], \"next_step\": \"chat\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entscheidungsfunktion für den Workflow\n",
    "def router(state: ConversationState) -> Literal[\"chat\", \"database\", \"end\"]:\n",
    "    return state[\"next_step\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph erstellen\n",
    "workflow = StateGraph(ConversationState)\n",
    "\n",
    "# Knoten hinzufügen\n",
    "workflow.add_node(\"chat\", chat_node)\n",
    "workflow.add_node(\"database\", database_query)\n",
    "\n",
    "# Startpunkt festlegen\n",
    "workflow.set_entry_point(\"chat\")\n",
    "\n",
    "# Kanten mit Bedingungen hinzufügen\n",
    "workflow.add_conditional_edges(\n",
    "    \"chat\",\n",
    "    router,\n",
    "    {\n",
    "        \"database\": \"database\",\n",
    "        \"end\": END,\n",
    "        \"chat\": \"chat\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Verbindung von Datenbank zurück zum Chat\n",
    "workflow.add_conditional_edges(\n",
    "    \"database\",\n",
    "    router,\n",
    "    {\n",
    "        \"chat\": \"chat\",\n",
    "        \"end\": END,\n",
    "        \"database\": \"database\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Graph kompilieren\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph ausführen\n",
    "result = graph.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"Hallo, ich suche Informationen zu maschinellem Lernen. Kannst du in der Datenbank nach Ressourcen suchen?\")],\n",
    "    \"next_step\": \"chat\"\n",
    "})\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "for message in result[\"messages\"]:\n",
    "    print(f\"{message.type}: {message.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Praxisübung: Chat-Anwendung mit Gedächtnis\n",
    "\n",
    "Erstellen Sie eine einfache Chat-Anwendung, die sich an Benutzerpräferenzen erinnert und entsprechend reagiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "\n",
    "# Prompt-Template mit Memory-Integration\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein hilfreicher Assistent, der sich an die Vorlieben und Informationen des Nutzers erinnert.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),  # Platzhalter für den Chat-Verlauf\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Erstellen der Konversationskette mit Buffer Memory\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n",
    "chain = prompt | llm\n",
    "\n",
    "\n",
    "# Funktion für die Chat-Interaktion\n",
    "def chat(user_input):\n",
    "    result = chain.invoke({\n",
    "        \"input\": user_input,\n",
    "        \"chat_history\": memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "    })\n",
    "\n",
    "    # Speichern der Interaktion im Gedächtnis\n",
    "    memory.save_context({\"input\": user_input}, {\"output\": result.content})\n",
    "\n",
    "    return result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel-Interaktion\n",
    "print(\"Assistant: \" + chat(\"Hallo, ich bin Stefan und komme aus München.\"))\n",
    "print(\"\\nAssistant: \" + chat(\"Ich mag Wandern und italienisches Essen.\"))\n",
    "print(\"\\nAssistant: \" + chat(\"Kannst du mir eine Aktivität für das Wochenende empfehlen?\"))\n",
    "print(\"\\nAssistant: \" + chat(\"Wie heißt du nochmal und woher komme ich?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Übungsaufgaben\n",
    "\n",
    "1. **Einfache Übung**: Modifizieren Sie die ConversationBufferMemory, um nur die letzten 3 Nachrichten zu speichern.\n",
    "\n",
    "2. **Mittlere Übung**: Implementieren Sie eine Chat-Anwendung mit ConversationSummaryBufferMemory, die automatisch zusammenfasst, wenn der Kontext zu lang wird.\n",
    "\n",
    "3. **Fortgeschrittene Übung**: Erweitern Sie den LangGraph-Workflow um einen zusätzlichen Knoten, der Benutzerpräferenzen in einer separaten Datenstruktur speichert und bei Bedarf abruft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Zusammenfassung\n",
    "\n",
    "- State Management ist entscheidend für die Entwicklung natürlicher und nützlicher KI-Anwendungen\n",
    "- LangChain bietet verschiedene Memory-Typen für unterschiedliche Anwendungsfälle:\n",
    "  - ConversationBufferMemory für einfache Konversationen\n",
    "  - ConversationSummaryMemory für längere Gespräche\n",
    "  - VectorStoreMemory für semantische Suche in der Konversationshistorie\n",
    "- LangGraph erweitert die Möglichkeiten durch typisierte Zustände und komplexe Workflows\n",
    "- Die Wahl des richtigen Memory-Typs hängt von den spezifischen Anforderungen der Anwendung ab\n",
    "\n",
    "In der Praxis werden oft Kombinationen verschiedener Techniken verwendet, um optimale Ergebnisse zu erzielen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
