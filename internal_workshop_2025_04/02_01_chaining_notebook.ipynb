{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaining - Verketten von Anfragen und Modellen\n",
    "\n",
    "In diesem Notebook lernen wir das Konzept des Chainings in LangChain kennen. Chaining erlaubt uns die Verkettung mehrerer Komponenten für komplexe Workflow-Muster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import der benötigten Bibliotheken\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from helpers import llm\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Grundlagen des Chainings\n",
    "\n",
    "Chaining ist ein grundlegendes Konzept in LangChain, bei dem verschiedene Komponenten miteinander verkettet werden, um Daten in einer Pipeline zu verarbeiten. Die LangChain Expression Language (LCEL) bietet eine elegante Syntax mit dem Pipe-Operator `|` für solche Verkettungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ein einfaches Beispiel für Chaining\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein hilfreicher Assistent für {beruf}.\"),\n",
    "    (\"human\", \"Erkläre in drei Sätzen, warum {thema} wichtig für Deine Tätigkeit ist.\")\n",
    "])\n",
    "\n",
    "# Einfache Chain mit Pipe-Operator\n",
    "chain = prompt | llm() | StrOutputParser()\n",
    "\n",
    "# Chain ausführen\n",
    "result = chain.invoke({\"beruf\": \"Programmierer\", \"thema\": \"Versionskontrolle\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Der Pipe-Operator und LCEL\n",
    "\n",
    "Der Pipe-Operator (`|`) ist das zentrale Element der LangChain Expression Language (LCEL). Er ermöglicht das Verketten von Komponenten auf intuitive Weise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorteile des Pipe-Operators demonstrieren\n",
    "\n",
    "# Beispiel 1: Einfache Verkettung\n",
    "einfache_chain = prompt | llm() | StrOutputParser()\n",
    "\n",
    "# Beispiel 2: Alternative Schreibweise ohne Pipe-Operator (umständlicher)\n",
    "def ohne_pipe_operator(beruf, thema):\n",
    "    formatted_prompt = prompt.format(beruf=beruf, thema=thema)\n",
    "    llm_response = llm().invoke(formatted_prompt)\n",
    "    parsed_response = StrOutputParser().invoke(llm_response)\n",
    "    return parsed_response\n",
    "\n",
    "# Vergleichen der Ergebnisse\n",
    "pipe_result = einfache_chain.invoke({\"beruf\": \"Arzt\", \"thema\": \"Empathie\"})\n",
    "traditional_result = ohne_pipe_operator(\"Arzt\", \"Empathie\")\n",
    "\n",
    "print(\"Mit Pipe-Operator:\\n\", pipe_result)\n",
    "print(\"\\nOhne Pipe-Operator:\\n\", traditional_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Streaming mit LCEL\n",
    "\n",
    "Ein großer Vorteil des LCEL-Ansatzes ist die integrierte Unterstützung für Streaming, was besonders für längere LLM-Antworten nützlich ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming mit LCEL demonstrieren\n",
    "geschichten_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein talentierter Geschichtenerzähler.\"),\n",
    "    (\"human\", \"Erzähle eine kurze Geschichte (etwa 200 Wörter) über {protagonist}, der/die {handlung}.\")\n",
    "])\n",
    "\n",
    "geschichten_chain = geschichten_prompt | llm() | StrOutputParser()\n",
    "\n",
    "# Normale Ausgabe zum Vergleich\n",
    "print(\"=== Normale Ausgabe (vollständig) ===\\n\")\n",
    "normal_result = geschichten_chain.invoke({\"protagonist\": \"ein Roboter\", \"handlung\": \"Gefühle entwickelt\"})\n",
    "print(normal_result)\n",
    "\n",
    "# Streaming-Ausgabe (asynchron)\n",
    "print(\"\\n=== Streaming-Ausgabe (Zeichen für Zeichen) ===\\n\")\n",
    "\n",
    "async def stream_text():\n",
    "    async for chunk in geschichten_chain.astream({\"protagonist\": \"eine KI\", \"handlung\": \"die Welt entdeckt\"}):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        await asyncio.sleep(0.01)  # Leichte Verzögerung für den Streaming-Effekt\n",
    "    print()  # Neue Zeile am Ende\n",
    "\n",
    "# In Jupyter ausführen\n",
    "await stream_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Von einfachen zu komplexen Chains\n",
    "\n",
    "LCEL ermöglicht es uns, über einfache sequentielle Chains hinaus zu gehen und komplexere Workflow-Muster zu erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Komplexe Chain mit sequenzieller Verarbeitung\n",
    "zusammenfassung_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein Experte für prägnante Zusammenfassungen.\"),\n",
    "    (\"human\", \"Fasse den folgenden Text in maximal 3 Sätzen zusammen:\\n\\n{text}\")\n",
    "])\n",
    "\n",
    "übersetzung_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein professioneller Übersetzer.\"),\n",
    "    (\"human\", \"Übersetze den folgenden Text ins {zielsprache}:\\n\\n{text}\")\n",
    "])\n",
    "\n",
    "# Einzelne Chains\n",
    "zusammenfassung_chain = zusammenfassung_prompt | llm() | StrOutputParser()\n",
    "übersetzung_chain = übersetzung_prompt | llm() | StrOutputParser()\n",
    "\n",
    "# Kombinierte Chain mit manuellem Input-Mapping\n",
    "def zusammenfassen_und_übersetzen(text, zielsprache):\n",
    "    # Erst zusammenfassen\n",
    "    zusammenfassung = zusammenfassung_chain.invoke({\"text\": text})\n",
    "    \n",
    "    # Dann die Zusammenfassung übersetzen\n",
    "    übersetzung = übersetzung_chain.invoke({\"text\": zusammenfassung, \"zielsprache\": zielsprache})\n",
    "    \n",
    "    return {\n",
    "        \"original\": text,\n",
    "        \"zusammenfassung\": zusammenfassung,\n",
    "        \"übersetzung\": übersetzung\n",
    "    }\n",
    "\n",
    "# Beispieltext\n",
    "langer_text = \"\"\"\n",
    "Large Language Models (LLMs) sind eine Art von künstlicher Intelligenz, die auf umfangreichen \n",
    "Trainingsdaten basierend natürliche Sprache verarbeiten und generieren können. Diese Modelle \n",
    "nutzen komplexe neuronale Netzwerke, insbesondere Transformer-Architekturen, um Muster in Sprache \n",
    "zu erkennen und zu reproduzieren. LLMs wie GPT-4, Claude und LLaMA können verschiedene Aufgaben \n",
    "wie Textgenerierung, Übersetzung, Zusammenfassung und Beantwortung von Fragen übernehmen. \n",
    "Ein entscheidender Faktor für ihre Leistung ist die Größe des Modells, gemessen an der Anzahl \n",
    "der Parameter, sowie die Qualität und Vielfalt der Trainingsdaten. Trotz ihrer beeindruckenden \n",
    "Fähigkeiten haben LLMs auch Limitierungen, darunter das Risiko, fehlerhafte Informationen zu \n",
    "produzieren (\"Halluzinationen\"), potenzielle Verzerrungen aus den Trainingsdaten und \n",
    "Schwierigkeiten bei der Handhabung von Kontextinformationen über lange Sequenzen hinweg.\n",
    "\"\"\"\n",
    "\n",
    "# Chain ausführen\n",
    "ergebnis = zusammenfassen_und_übersetzen(langer_text, \"Spanisch\")\n",
    "\n",
    "print(\"=== Original ===\\n\")\n",
    "print(langer_text)\n",
    "\n",
    "print(\"\\n=== Zusammenfassung ===\\n\")\n",
    "print(ergebnis[\"zusammenfassung\"])\n",
    "\n",
    "print(\"\\n=== Übersetzung der Zusammenfassung ins Spanische ===\\n\")\n",
    "print(ergebnis[\"übersetzung\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parallele Verarbeitung mit RunnableMap\n",
    "\n",
    "Mit `RunnableMap` können wir mehrere Verarbeitungspfade parallel ausführen und die Ergebnisse zusammenführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableMap\n",
    "\n",
    "# Multilinguale Übersetzung mit paralleler Verarbeitung\n",
    "übersetzung_chain_mit_sprache = lambda sprache: übersetzung_prompt | llm() | StrOutputParser()\n",
    "\n",
    "# Mehrere Sprachen parallel übersetzen\n",
    "multi_übersetzung = RunnableMap({\n",
    "    \"original\": lambda x: x[\"text\"],\n",
    "    \"deutsch\": lambda x: übersetzung_chain.invoke({\"text\": x[\"text\"], \"zielsprache\": \"Deutsch\"}),\n",
    "    \"englisch\": lambda x: übersetzung_chain.invoke({\"text\": x[\"text\"], \"zielsprache\": \"Englisch\"}),\n",
    "    \"spanisch\": lambda x: übersetzung_chain.invoke({\"text\": x[\"text\"], \"zielsprache\": \"Spanisch\"}),\n",
    "    \"französisch\": lambda x: übersetzung_chain.invoke({\"text\": x[\"text\"], \"zielsprache\": \"Französisch\"})\n",
    "})\n",
    "\n",
    "# Beispiel für parallele Übersetzung\n",
    "kurzer_text = \"Künstliche Intelligenz verändert die Art, wie wir arbeiten, kommunizieren und leben.\"\n",
    "\n",
    "# Chain ausführen\n",
    "übersetzungen = multi_übersetzung.invoke({\"text\": kurzer_text})\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "for sprache, text in übersetzungen.items():\n",
    "    if sprache != \"original\":\n",
    "        print(f\"=== {sprache.capitalize()} ===\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bedingte Verzweigungen mit RunnableBranch\n",
    "\n",
    "Mit `RunnableBranch` können wir basierend auf bestimmten Bedingungen unterschiedliche Verarbeitungspfade wählen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "# Klassifikation für Verzweigungen nutzen\n",
    "klassifikation_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein Textklassifikator. Wähle genau EINE der folgenden Kategorien für den Text: TECHNISCH, GESCHÄFTLICH, KREATIV.\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "klassifikation_chain = klassifikation_prompt | llm() | StrOutputParser()\n",
    "\n",
    "# Spezialisierte Aufgaben je nach Texttyp\n",
    "technischer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein technischer Experte. Erkläre das folgende Konzept detailliert und technisch präzise.\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "geschäftlicher_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein Business-Analyst. Analysiere die geschäftlichen Implikationen des folgenden Themas.\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "kreativer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein kreativer Autor. Schreibe eine inspirierende Geschichte basierend auf dem folgenden Thema.\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "# Chains für verschiedene Texttypen\n",
    "technische_chain = technischer_prompt | llm() | StrOutputParser()\n",
    "geschäftliche_chain = geschäftlicher_prompt | llm() | StrOutputParser()\n",
    "kreative_chain = kreativer_prompt | llm() | StrOutputParser()\n",
    "\n",
    "# Verzweigungslogik mit RunnableBranch\n",
    "bedingte_chain = RunnableBranch(\n",
    "    (lambda x: \"TECHNISCH\" in klassifikation_chain.invoke({\"text\": x[\"text\"]}).upper(), \n",
    "     lambda x: {\"kategorie\": \"Technisch\", \"antwort\": technische_chain.invoke({\"text\": x[\"text\"]})}),\n",
    "    \n",
    "    (lambda x: \"GESCHÄFT\" in klassifikation_chain.invoke({\"text\": x[\"text\"]}).upper(), \n",
    "     lambda x: {\"kategorie\": \"Geschäftlich\", \"antwort\": geschäftliche_chain.invoke({\"text\": x[\"text\"]})}),\n",
    "    \n",
    "    # Fallback für alle anderen Kategorien (Standard: Kreativ)\n",
    "    lambda x: {\"kategorie\": \"Kreativ\", \"antwort\": kreative_chain.invoke({\"text\": x[\"text\"]})}\n",
    ")\n",
    "\n",
    "# Beispieltexte für verschiedene Kategorien\n",
    "texte = [\n",
    "    \"Wie funktionieren Transformer-Modelle in der künstlichen Intelligenz?\",\n",
    "    \"Welche Strategien sollten Unternehmen verfolgen, um von generativer KI zu profitieren?\",\n",
    "    \"Eine Welt, in der KI und Menschen harmonisch zusammenarbeiten\"\n",
    "]\n",
    "\n",
    "# Chains für jeden Text ausführen\n",
    "for i, text in enumerate(texte):\n",
    "    print(f\"\\n=== Beispiel {i+1} ===\\n\")\n",
    "    print(f\"Text: {text}\")\n",
    "    \n",
    "    ergebnis = bedingte_chain.invoke({\"text\": text})\n",
    "    \n",
    "    print(f\"\\nKategorie: {ergebnis['kategorie']}\")\n",
    "    print(f\"\\nAntwort: {ergebnis['antwort'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Praxisübung: Text-Analyse-Pipeline erstellen\n",
    "\n",
    "Erstellen Sie eine umfassende Pipeline, die einen Text analysiert, zusammenfasst und wichtige Erkenntnisse extrahiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Pydantic-Modell für strukturierte Ausgabe\n",
    "class TextAnalyse(BaseModel):\n",
    "    hauptthemen: List[str] = Field(description=\"Die wichtigsten Themen im Text\")\n",
    "    stimmung: str = Field(description=\"Die allgemeine Stimmung des Textes (positiv, neutral, negativ)\")\n",
    "    schlüsselwörter: List[str] = Field(description=\"Wichtige Schlüsselwörter im Text\")\n",
    "    zielgruppe: str = Field(description=\"Die wahrscheinliche Zielgruppe des Textes\")\n",
    "\n",
    "# Parser für strukturierte Ausgabe\n",
    "parser = PydanticOutputParser(pydantic_object=TextAnalyse)\n",
    "\n",
    "# Prompts für die verschiedenen Schritte der Pipeline\n",
    "# 1. Zusammenfassung\n",
    "zusammenfassung_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Fasse den folgenden Text in 2-3 prägnanten Sätzen zusammen.\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "# 2. Detaillierte Analyse\n",
    "analyse_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Analysiere den folgenden Text und extrahiere strukturierte Informationen.\\n\\n{format_instructions}\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "# 3. Empfehlungen basierend auf der Analyse\n",
    "empfehlungen_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein Content-Stratege. Basierend auf der folgenden Textanalyse, gib 3 konkrete Empfehlungen, wie der Inhalt verbessert werden könnte.\"),\n",
    "    (\"human\", \"Zusammenfassung: {zusammenfassung}\\n\\nAnalyse: {analyse}\")\n",
    "])\n",
    "\n",
    "# Chains für die einzelnen Schritte\n",
    "zusammenfassung_chain = zusammenfassung_prompt | llm() | StrOutputParser()\n",
    "analyse_chain = analyse_prompt.partial(format_instructions=parser.get_format_instructions()) | llm() | parser\n",
    "empfehlungen_chain = empfehlungen_prompt | llm() | StrOutputParser()\n",
    "\n",
    "# Komplette Analyse-Pipeline\n",
    "def text_analyse_pipeline(text):\n",
    "    # Schritt 1: Zusammenfassung\n",
    "    zusammenfassung = zusammenfassung_chain.invoke({\"text\": text})\n",
    "    \n",
    "    # Schritt 2: Detaillierte Analyse\n",
    "    analyse = analyse_chain.invoke({\"text\": text})\n",
    "    \n",
    "    # Schritt 3: Empfehlungen basierend auf Zusammenfassung und Analyse\n",
    "    empfehlungen = empfehlungen_chain.invoke({\n",
    "        \"zusammenfassung\": zusammenfassung,\n",
    "        \"analyse\": analyse.model_dump_json()\n",
    "    })\n",
    "    \n",
    "    # Ergebnisse zusammenführen\n",
    "    return {\n",
    "        \"zusammenfassung\": zusammenfassung,\n",
    "        \"analyse\": analyse,\n",
    "        \"empfehlungen\": empfehlungen\n",
    "    }\n",
    "\n",
    "# Beispieltext für die Pipeline\n",
    "beispieltext = \"\"\"\n",
    "Künstliche Intelligenz hat in den letzten Jahren enorme Fortschritte gemacht, insbesondere im Bereich der großen Sprachmodelle. \n",
    "Diese Technologie bietet zahlreiche Vorteile für Unternehmen, von Automatisierung bis hin zur Verbesserung der Kundenerfahrung. \n",
    "Allerdings müssen Organisationen auch die ethischen Implikationen und potenziellen Risiken berücksichtigen. \n",
    "Der verantwortungsvolle Einsatz von KI erfordert klare Richtlinien und kontinuierliche Überwachung. \n",
    "Trotz der Herausforderungen werden Unternehmen, die diese Technologie effektiv nutzen, einen bedeutenden Wettbewerbsvorteil erlangen.\n",
    "\"\"\"\n",
    "\n",
    "# Pipeline ausführen\n",
    "analyseergebnis = text_analyse_pipeline(beispieltext)\n",
    "\n",
    "# Ergebnisse formatiert ausgeben\n",
    "print(\"=== Zusammenfassung ===\\n\")\n",
    "print(analyseergebnis[\"zusammenfassung\"])\n",
    "\n",
    "print(\"\\n=== Detaillierte Analyse ===\\n\")\n",
    "print(f\"Hauptthemen: {', '.join(analyseergebnis['analyse'].hauptthemen)}\")\n",
    "print(f\"Stimmung: {analyseergebnis['analyse'].stimmung}\")\n",
    "print(f\"Schlüsselwörter: {', '.join(analyseergebnis['analyse'].schlüsselwörter)}\")\n",
    "print(f\"Zielgruppe: {analyseergebnis['analyse'].zielgruppe}\")\n",
    "\n",
    "print(\"\\n=== Empfehlungen ===\\n\")\n",
    "print(analyseergebnis[\"empfehlungen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Übung für Teilnehmer\n",
    "\n",
    "**Aufgabe**: Erstellen Sie eine Chain, die:\n",
    "1. Einen englischen Text entgegennimmt\n",
    "2. Diesen ins Deutsche übersetzt\n",
    "3. Eine Zusammenfassung erstellt\n",
    "4. Diese Zusammenfassung in einen Twitter-Post (max. 240 Zeichen) umwandelt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier Ihre Lösung implementieren\n",
    "\n",
    "# Hilfestellung:\n",
    "# 1. Definieren Sie die nötigen Prompts für jeden Schritt\n",
    "# 2. Erstellen Sie Chains für jeden einzelnen Schritt\n",
    "# 3. Verbinden Sie die Chains zu einer Gesamtpipeline\n",
    "# 4. Testen Sie Ihre Chain mit einem englischen Beispieltext\n",
    "\n",
    "# Beispiel für den Beginn der Lösung:\n",
    "übersetzung_en_de_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein professioneller Übersetzer für Englisch nach Deutsch.\"),\n",
    "    (\"human\", \"Übersetze den folgenden englischen Text ins Deutsche:\\n\\n{text}\")\n",
    "])\n",
    "\n",
    "# TODO: Weitere Prompts und Chains definieren\n",
    "\n",
    "# TODO: Pipeline implementieren\n",
    "\n",
    "# Beispieltext zum Testen\n",
    "englischer_text = \"\"\"\n",
    "Artificial intelligence has rapidly evolved in recent years, transforming various industries \n",
    "and creating new opportunities for innovation. Machine learning models, particularly large \n",
    "language models, have demonstrated impressive capabilities in understanding and generating \n",
    "human language. However, these advances also raise important questions about ethics, privacy, \n",
    "and the future of work in an increasingly automated world.\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Pipeline ausführen und Ergebnisse anzeigen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Zusammenfassung\n",
    "\n",
    "In diesem Notebook haben wir gelernt:\n",
    "- Wie man mithilfe des Pipe-Operators (`|`) einfache und komplexe Chains in LangChain erstellt\n",
    "- Wie die LangChain Expression Language (LCEL) funktioniert und ihre Vorteile\n",
    "- Wie man Streaming für eine bessere Benutzererfahrung nutzen kann\n",
    "- Wie man komplexe Workflows mit sequenzieller Verarbeitung, parallelen Verarbeitungspfaden und bedingten Verzweigungen erstellt\n",
    "- Wie man praktische Anwendungen wie mehrsprachige Übersetzung und Textanalyse mit Chains umsetzt\n",
    "\n",
    "Diese Chainings-Techniken bilden die Grundlage für fortgeschrittenere Architekturen, die wir in den nächsten Abschnitten des Workshops kennenlernen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-agents-Wmcp_RVb-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
