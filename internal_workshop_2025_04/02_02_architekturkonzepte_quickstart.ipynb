{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architekturkonzepte: Quick Start Guide\n",
    "\n",
    "Dieses Notebook bietet einen schnellen Einstieg in verschiedene Architekturkonzepte für LLM-Anwendungen, die im Workshop vorgestellt wurden. Verwenden Sie diese Beispiele, um die Konzepte schnell in eigenen Projekten anzuwenden.\n",
    "\n",
    "## Inhalt\n",
    "1. Einfacher Agent\n",
    "2. ReAct-Pattern\n",
    "3. Graph-RAG\n",
    "4. Hierarchical RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Benötigte Bibliotheken installieren\n",
    "!pip install langchain langchain-community langchain-openai langgraph python-dotenv -q\n",
    "\n",
    "# Umgebungsvariablen laden\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# API-Schlüssel überprüfen\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"✓ OpenAI API-Schlüssel konfiguriert\")\n",
    "else:\n",
    "    print(\"⚠ OpenAI API-Schlüssel fehlt! Bitte in .env konfigurieren.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Einfacher Agent\n",
    "\n",
    "Dieses Beispiel zeigt, wie Sie einen einfachen Agenten erstellen, der Tools verwenden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "\n",
    "# LLM initialisieren\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Tools laden\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "\n",
    "\n",
    "# Eigenes Tool erstellen\n",
    "@tool\n",
    "def wetter_info(ort: str) -> str:\n",
    "    \"\"\"Gibt Wetterinformationen für einen bestimmten Ort zurück. Der Ort sollte als Eingabe übergeben werden.\"\"\"\n",
    "    # In einem realen Szenario würden Sie hier eine Wetter-API abfragen\n",
    "    # Dies ist nur ein Beispiel\n",
    "    return f\"In {ort} sind es heute 22°C und sonnig.\"\n",
    "\n",
    "\n",
    "# Tool zur Liste hinzufügen\n",
    "tools.append(wetter_info)\n",
    "\n",
    "# Agent initialisieren\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Agent testen\n",
    "agent.invoke(\"Wie ist das Wetter in Berlin und berechne 25 hoch 0.5\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ReAct Pattern\n",
    "\n",
    "Das ReAct-Muster (Reasoning and Acting) ermöglicht dem LLM, sein Denken explizit zu verbalisieren und rational Entscheidungen zu treffen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ReAct-Agent für komplexe Aufgaben\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# LLM initialisieren\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Tools laden\n",
    "# Wir verwenden hier serpapi, falls konfiguriert; ansonsten DuckDuckGo\n",
    "search_tools = []\n",
    "if os.getenv(\"SERPAPI_API_KEY\"):\n",
    "    search_tools = load_tools([\"serpapi\"])\n",
    "else:\n",
    "    search_tools = load_tools([\"ddg-search\"])\n",
    "\n",
    "# Mathematik-Tool hinzufügen\n",
    "tools = search_tools + load_tools([\"llm-math\"], llm=llm)\n",
    "\n",
    "# ReAct-Agent initialisieren\n",
    "react_agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Agent mit einer komplexen Anfrage testen, die mehrere Schritte erfordert\n",
    "react_agent.invoke(\"Wer ist der aktuelle Bundeskanzler und in welchem Jahr wurde er geboren? Addiere dann 25 zu diesem Jahr.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph-RAG\n",
    "\n",
    "Graph-RAG kombiniert Graphstrukturen mit Retrieval Augmented Generation, um Beziehungen zwischen Dokumenten zu modellieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "# Zustandsdefinition für Graph-RAG\n",
    "class GraphState(TypedDict):\n",
    "    query: str\n",
    "    context: List[str]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# LLM und Embeddings initialisieren\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Beispieldaten für die Vektordatenbank\n",
    "sample_texts = [\n",
    "    \"Berlin ist die Hauptstadt von Deutschland und hat etwa 3,7 Millionen Einwohner.\",\n",
    "    \"München ist die Hauptstadt von Bayern und bekannt für das Oktoberfest.\",\n",
    "    \"Hamburg ist die zweitgrößte Stadt Deutschlands und ein wichtiger Hafen.\",\n",
    "    \"Frankfurt am Main ist ein bedeutendes Finanzzentrum in Europa.\",\n",
    "    \"Köln ist bekannt für seinen gotischen Dom und den Karneval.\"\n",
    "]\n",
    "\n",
    "# Vektordatenbank erstellen\n",
    "vectorstore = Chroma.from_texts(sample_texts, embeddings)\n",
    "\n",
    "\n",
    "# Knoten-Funktionen definieren\n",
    "def retrieve(state: GraphState) -> GraphState:\n",
    "    \"\"\"Dokumente aus der Vektordatenbank abrufen\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    docs = vectorstore.similarity_search(query, k=2)\n",
    "    return {\"context\": [doc.page_content for doc in docs]}\n",
    "\n",
    "\n",
    "def generate_answer(state: GraphState) -> GraphState:\n",
    "    \"\"\"Antwort basierend auf dem Kontext generieren\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Du bist ein hilfreicher Assistent. \n",
    "        Verwende den folgenden Kontext, um die Frage zu beantworten.\n",
    "        \n",
    "        Kontext: {context}\n",
    "        \n",
    "        Frage: {query}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    answer = chain.invoke({\"context\": \"\\n\".join(context), \"query\": query})\n",
    "\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "\n",
    "# Graph erstellen\n",
    "graph = StateGraph(GraphState)\n",
    "\n",
    "# Knoten hinzufügen\n",
    "graph.add_node(\"retrieve\", retrieve)\n",
    "graph.add_node(\"generate\", generate_answer)\n",
    "\n",
    "# Kanten definieren\n",
    "graph.set_entry_point(\"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"generate\")\n",
    "graph.set_finish_point(\"generate\")\n",
    "\n",
    "# Graph kompilieren und ausführen\n",
    "chain = graph.compile()\n",
    "result = chain.invoke({\"query\": \"Was ist die Hauptstadt von Deutschland?\", \"context\": [], \"answer\": \"\"})\n",
    "print(\"\\nAntwort:\", result[\"answer\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hierarchical RAG\n",
    "\n",
    "Hierarchical RAG organisiert Informationen in Hierarchien, um den Suchraum effizient einzugrenzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM initialisieren\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Hierarchische Dokument-Struktur (vereinfacht)\n",
    "document_hierarchy = {\n",
    "    \"level1\": [\n",
    "        {\"id\": \"doc1\", \"summary\": \"Deutschlands Großstädte und ihre Bedeutung\"},\n",
    "        {\"id\": \"doc2\", \"summary\": \"Europäische Hauptstädte im Vergleich\"}\n",
    "    ],\n",
    "    \"level2\": {\n",
    "        \"doc1\": [\n",
    "            {\"id\": \"doc1_section1\", \"title\": \"Berlin als Hauptstadt\"},\n",
    "            {\"id\": \"doc1_section2\", \"title\": \"Hamburg als Handelsmetropole\"}\n",
    "        ],\n",
    "        \"doc2\": [\n",
    "            {\"id\": \"doc2_section1\", \"title\": \"Berlin im europäischen Kontext\"},\n",
    "            {\"id\": \"doc2_section2\", \"title\": \"Paris als Kulturzentrum\"}\n",
    "        ]\n",
    "    },\n",
    "    \"level3\": {\n",
    "        \"doc1_section1\": \"Berlin ist die Hauptstadt und bevölkerungsreichste Stadt Deutschlands. Mit rund 3,7 Millionen Einwohnern ist Berlin auch die größte Stadt der Europäischen Union.\",\n",
    "        \"doc1_section2\": \"Hamburg ist mit 1,8 Millionen Einwohnern die zweitgrößte Stadt Deutschlands und ein bedeutendes Wirtschafts- und Handelszentrum in Nordeuropa.\",\n",
    "        \"doc2_section1\": \"Im Vergleich zu anderen europäischen Hauptstädten hat Berlin eine besondere Geschichte aufgrund der deutschen Teilung im 20. Jahrhundert.\",\n",
    "        \"doc2_section2\": \"Paris, die Hauptstadt Frankreichs, gilt als eines der wichtigsten Kulturzentren Europas mit berühmten Museen wie dem Louvre und dem Centre Pompidou.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def hierarchical_search(query: str) -> str:\n",
    "    \"\"\"Führt eine hierarchische Suche durch\"\"\"\n",
    "\n",
    "    # Level 1: Auswahl des relevanten Dokuments\n",
    "    level1_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Gegeben sind die folgenden Dokumentzusammenfassungen:\n",
    "        {summaries}\n",
    "        \n",
    "        Für die Anfrage: {query}\n",
    "        Gib die ID des relevantesten Dokuments zurück. Antworte nur mit der ID.\"\"\"\n",
    "    )\n",
    "\n",
    "    summaries = \"\\n\".join([f\"ID: {doc['id']}, Zusammenfassung: {doc['summary']}\"\n",
    "                           for doc in document_hierarchy[\"level1\"]])\n",
    "\n",
    "    level1_chain = level1_prompt | llm | StrOutputParser()\n",
    "    selected_doc = level1_chain.invoke({\"summaries\": summaries, \"query\": query})\n",
    "    print(f\"Ausgewähltes Dokument (Level 1): {selected_doc}\")\n",
    "\n",
    "    # Level 2: Auswahl des relevanten Abschnitts\n",
    "    level2_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Gegeben sind die folgenden Abschnitte aus dem Dokument {doc_id}:\n",
    "        {sections}\n",
    "        \n",
    "        Für die Anfrage: {query}\n",
    "        Gib die ID des relevantesten Abschnitts zurück. Antworte nur mit der ID.\"\"\"\n",
    "    )\n",
    "\n",
    "    sections = \"\\n\".join([f\"ID: {section['id']}, Titel: {section['title']}\"\n",
    "                          for section in document_hierarchy[\"level2\"][selected_doc]])\n",
    "\n",
    "    level2_chain = level2_prompt | llm | StrOutputParser()\n",
    "    selected_section = level2_chain.invoke({\"doc_id\": selected_doc, \"sections\": sections, \"query\": query})\n",
    "    print(f\"Ausgewählter Abschnitt (Level 2): {selected_section}\")\n",
    "\n",
    "    # Level 3: Abrufen des detaillierten Inhalts\n",
    "    content = document_hierarchy[\"level3\"][selected_section]\n",
    "\n",
    "    # Generieren der finalen Antwort\n",
    "    answer_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Basierend auf dem folgenden Text, beantworte die Frage.\n",
    "        \n",
    "        Text: {content}\n",
    "        \n",
    "        Frage: {query}\n",
    "        \n",
    "        Antwort:\"\"\"\n",
    "    )\n",
    "\n",
    "    answer_chain = answer_prompt | llm | StrOutputParser()\n",
    "    answer = answer_chain.invoke({\"content\": content, \"query\": query})\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "# Hierarchische Suche testen\n",
    "result = hierarchical_search(\"Was ist besonders an Berlin?\")\n",
    "print(\"\\nFinale Antwort:\")\n",
    "print(result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bonus: Kombinierte Architektur (ReAct + Graph-RAG)\n",
    "\n",
    "Dieses Beispiel zeigt, wie Sie ReAct und Graph-RAG kombinieren können, um einen leistungsfähigen Agenten zu erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# LLM und Embeddings initialisieren\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Beispieldaten für die Vektordatenbank\n",
    "sample_texts = [\n",
    "    \"Berlin ist die Hauptstadt von Deutschland und hat etwa 3,7 Millionen Einwohner.\",\n",
    "    \"München ist die Hauptstadt von Bayern und bekannt für das Oktoberfest.\",\n",
    "    \"Hamburg ist mit 1,8 Millionen Einwohnern die zweitgrößte Stadt Deutschlands.\",\n",
    "    \"Frankfurt am Main ist ein bedeutendes Finanzzentrum in Europa.\",\n",
    "    \"Köln ist bekannt für seinen gotischen Dom und den Karneval.\"\n",
    "]\n",
    "\n",
    "# Vektordatenbank erstellen\n",
    "vectorstore = Chroma.from_texts(sample_texts, embeddings)\n",
    "\n",
    "\n",
    "# Tool für Dokumentensuche erstellen\n",
    "@tool\n",
    "def suche_dokument(query: str) -> str:\n",
    "    \"\"\"Sucht nach relevanten Dokumenten basierend auf einer Anfrage.\"\"\"\n",
    "    docs = vectorstore.similarity_search(query, k=2)\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "\n",
    "# Tool für mathematische Berechnungen\n",
    "math_tools = load_tools([\"llm-math\"], llm=llm)\n",
    "\n",
    "# Alle Tools kombinieren\n",
    "tools = [suche_dokument] + math_tools\n",
    "\n",
    "# ReAct-Agent erstellen, der die Tools verwendet\n",
    "combined_agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Agent testen mit einer Anfrage, die sowohl Dokumentensuche als auch Mathematik erfordert\n",
    "combined_agent.invoke(\n",
    "    \"Welche Stadt ist die bevölkerungsreichste in Deutschland und wie viel ist ihre Einwohnerzahl in Millionen zum Quadrat?\"\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In diesem Quick Start Guide haben Sie kennengelernt:\n",
    "- Wie Sie einfache Agenten mit Tools erstellen\n",
    "- Wie das ReAct-Pattern funktioniert und implementiert wird\n",
    "- Wie Graph-RAG mit LangGraph umgesetzt wird\n",
    "- Wie hierarchische Suche in strukturierten Dokumenten funktioniert\n",
    "- Wie Sie verschiedene Architekturansätze kombinieren können\n",
    "\n",
    "Diese Beispiele dienen als Ausgangspunkt für Ihre eigenen LLM-Anwendungen. Experimentieren Sie mit verschiedenen Kombinationen und passen Sie die Beispiele an Ihre speziellen Anforderungen an."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
