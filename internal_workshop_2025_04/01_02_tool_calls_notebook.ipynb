{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool Calls mit LangChain und LLMs\n",
    "\n",
    "In diesem Notebook lernen wir, wie wir eigene Tools erstellen und in LLMs integrieren können. Tool Calls ermöglichen es, die Fähigkeiten von LLMs durch externe Funktionen und Dienste zu erweitern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Grundlegende Importe und Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from helpers import llm\n",
    "from langchain.tools import tool\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.messages import BaseMessage\n",
    "import json\n",
    "\n",
    "# OpenAI API Wrapper erstellen\n",
    "model = llm(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ein einfaches Tool erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def personen_info(name: str):\n",
    "    \"\"\"Gibt Informationen über eine Person zurück.\n",
    "    \n",
    "    Args:\n",
    "        name: Der Name der Person\n",
    "        \n",
    "    Returns:\n",
    "        Informationen über die Person\n",
    "    \"\"\"\n",
    "    personen = {\n",
    "        \"fritz karuugaa\": \"Fritz Karuugaa ist ein Software-Entwickler.\",\n",
    "        \"albert einstein\": \"Albert Einstein (1879-1955) war ein theoretischer Physiker und Entwickler der Relativitätstheorie.\",\n",
    "        \"marie curie\": \"Marie Curie (1867-1934) war eine Physikerin und Chemikerin, die zweimal den Nobelpreis erhielt.\"\n",
    "    }\n",
    "    \n",
    "    return personen.get(name.lower(), f\"Keine Informationen über {name} verfügbar.\")\n",
    "\n",
    "# Testen des Tools\n",
    "print(personen_info.invoke(\"Fritz Karuugaa\"))\n",
    "print(personen_info.invoke(\"Albert Einstein\"))\n",
    "print(personen_info.invoke(\"Marie Curie\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tools an ein LLM binden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools als Liste bereitstellen (auch wenn es nur eines ist)\n",
    "tools = [personen_info]\n",
    "\n",
    "# Tools an das LLM binden\n",
    "# Wichtig: Hierbei werden die Tool-Beschreibungen automatisch in den Kontext des LLMs eingefügt\n",
    "llm_with_tools = model.bind_tools(tools)\n",
    "\n",
    "# Prompt Template erstellen\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"Du bist ein hilfsbereicher Assistent, der Fragen zu Personen beantwortet.\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "        MessagesPlaceholder(\"intermediate_steps\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Einfache LangChain Expression Language (LCEL) Chain erstellen\n",
    "chain = prompt | llm_with_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Das Tool mit einer einfachen Anfrage testen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kleiner Helfer um Chain antworten schön anziegen zu lassen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response printer function\n",
    "def print_response(response):\n",
    "    if isinstance(response, BaseMessage):\n",
    "        response.pretty_print()\n",
    "    elif isinstance(response, (str, list, dict)):\n",
    "        print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einfache Anfrage an das LLM mit Tool-Unterstützung\n",
    "response = chain.invoke({\"input\": \"Wer ist Marie Curie?\"})\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Währungsumrechnungs-Tool erstellen\n",
    "\n",
    "Jetzt implementieren wir ein Tool zur Währungsumrechnung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def waehrungsumrechnung(betrag: float, von_währung: str, zu_währung: str):\n",
    "    \"\"\"Rechnet einen Geldbetrag von einer Währung in eine andere um.\n",
    "    \n",
    "    Args:\n",
    "        betrag: Der umzurechnende Geldbetrag\n",
    "        von_währung: Quellwährung (z.B. \"EUR\", \"USD\", \"GBP\", \"JPY\")\n",
    "        zu_währung: Zielwährung (z.B. \"EUR\", \"USD\", \"GBP\", \"JPY\")\n",
    "        \n",
    "    Returns:\n",
    "        Eine Zeichenkette mit dem umgerechneten Betrag\n",
    "    \"\"\"\n",
    "    # Einfache Wechselkurse (in der Realität würde man eine API verwenden)\n",
    "    kurse = {\"EUR\": 1.0, \"USD\": 1.08, \"GBP\": 0.85, \"JPY\": 163.2, \"CHF\": 0.97}\n",
    "    \n",
    "    # Prüfen, ob die Währungen unterstützt werden\n",
    "    if von_währung not in kurse or zu_währung not in kurse:\n",
    "        return f\"Fehler: Eine der angegebenen Währungen wird nicht unterstützt. Verfügbare Währungen: {', '.join(kurse.keys())}\"\n",
    "    \n",
    "    # Umrechnung durchführen\n",
    "    ergebnis = betrag * (kurse[zu_währung] / kurse[von_währung])\n",
    "    \n",
    "    # Ergebnis zurückgeben\n",
    "    return f\"{betrag} {von_währung} = {ergebnis:.2f} {zu_währung}\"\n",
    "\n",
    "# Testen des Tools\n",
    "print(waehrungsumrechnung.invoke({\"betrag\": 100, \"von_währung\": \"EUR\", \"zu_währung\": \"USD\"}))\n",
    "print(waehrungsumrechnung.invoke({\"betrag\": 50, \"von_währung\": \"GBP\", \"zu_währung\": \"EUR\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mehrere Tools kombinieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wetterinformations-Tool erstellen\n",
    "@tool\n",
    "def wetter_info(ort: str):\n",
    "    \"\"\"Gibt Wetterinformationen für einen bestimmten Ort zurück.\n",
    "    \n",
    "    Args:\n",
    "        ort: Der Name der Stadt oder des Ortes\n",
    "        \n",
    "    Returns:\n",
    "        Eine Beschreibung des aktuellen Wetters\n",
    "    \"\"\"\n",
    "    # Beispieldaten (in der Realität würde man eine Wetter-API verwenden)\n",
    "    wetterdaten = {\n",
    "        \"berlin\": \"Berlin: 22°C, sonnig\",\n",
    "        \"hamburg\": \"Hamburg: 18°C, leicht bewölkt, leichter Wind\",\n",
    "        \"münchen\": \"München: 20°C, vereinzelte Wolken\",\n",
    "        \"köln\": \"Köln: 21°C, wolkenlos\",\n",
    "        \"frankfurt\": \"Frankfurt: 23°C, heiter bis wolkig\"\n",
    "    }\n",
    "    \n",
    "    return wetterdaten.get(ort.lower(), f\"Keine Wetterdaten für {ort} verfügbar.\")\n",
    "\n",
    "# Alle Tools kombinieren\n",
    "multi_tools = [personen_info, waehrungsumrechnung, wetter_info]\n",
    "\n",
    "# LLM mit allen Tools ausstatten\n",
    "llm_with_multi_tools = model.bind_tools(multi_tools)\n",
    "\n",
    "# Neues Prompt-Template erstellen\n",
    "multi_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"\"\"Du bist ein vielseitiger Assistent, der mit verschiedenen Werkzeugen ausgestattet ist, um Fragen zu beantworten.\n",
    "            Verwende das passende Werkzeug, um die Anfrage des Nutzers zu beantworten.\"\"\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "        MessagesPlaceholder(\"intermediate_steps\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Neue Chain erstellen\n",
    "multi_chain = multi_prompt | llm_with_multi_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test mit verschiedenen Anfragen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Personeninfo\n",
    "response1 = multi_chain.invoke({\"input\": \"Kannst du mir etwas über Albert Einstein sagen?\"})\n",
    "print(\"Antwort 1:\\n\")\n",
    "print(response1)\n",
    "print_response(response1)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Test 2: Währungsumrechnung\n",
    "response2 = multi_chain.invoke({\"input\": \"Wie viel sind 200 Euro in US-Dollar?\",\"intermediate_steps\":[]})\n",
    "print(\"Antwort 2:\\n\")\n",
    "print(response2)\n",
    "print_response(response2)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Test 3: Wetterinfo\n",
    "response3 = multi_chain.invoke({\"input\": \"Wie ist das Wetter in Berlin?\",\"intermediate_steps\":[]})\n",
    "print(\"Antwort 3:\\n\")\n",
    "print(response3)\n",
    "print_response(response3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Integration mit LangGraph\n",
    "\n",
    "Für komplexere Anwendungen können wir Tool Calls in einen LangGraph-Workflow integrieren. Hier zeigen wir einen einfachen Ansatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, TypedDict\n",
    "from langchain_core.agents import AgentActionMessageLog, AgentFinish\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# State für den Graphen definieren\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    intermediate_steps: Annotated[list[tuple[AgentActionMessageLog, str]], operator.add]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entscheidungsfunktion: Soll ein Tool ausgeführt werden oder sind wir fertig?\n",
    "def should_continue(state):\n",
    "    if not state[\"intermediate_steps\"]:\n",
    "        return \"continue\"\n",
    "    \n",
    "    last_step = state[\"intermediate_steps\"][-1]\n",
    "    if isinstance(last_step, AgentActionMessageLog) and last_step.tool_calls:\n",
    "        return \"continue\"\n",
    "    return \"end\"\n",
    "\n",
    "# Funktion zum Aufrufen des LLM/Agenten\n",
    "def call_model(state, config):\n",
    "    response = multi_chain.invoke(state, config=config)\n",
    "    \n",
    "    if isinstance(response, AgentFinish):\n",
    "        return {\"answer\": response.answer}\n",
    "    else:\n",
    "        return {\"intermediate_steps\": [response]}\n",
    "\n",
    "# Funktion zur Ausführung eines einzelnen Tools\n",
    "def _invoke_tool(tool_call):\n",
    "    tool_map = {tool.name: tool for tool in multi_tools}\n",
    "    tool = tool_map[tool_call[\"name\"]]\n",
    "    return ToolMessage(tool.invoke(tool_call[\"args\"]), tool_call_id=tool_call[\"id\"])\n",
    "\n",
    "# Wrapper für die Tool-Ausführung\n",
    "tool_executor = RunnableLambda(_invoke_tool)\n",
    "\n",
    "# Funktion zum Aufrufen aller benötigten Tools\n",
    "def call_tools(state):\n",
    "    last_message = state[\"intermediate_steps\"][-1]\n",
    "    return {\"intermediate_steps\": tool_executor.batch(last_message.tool_calls)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph-Workflow erstellen\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tools)\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# Bedingungen für Verzweigungen hinzufügen\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"action\",  # Tool ausführen\n",
    "        \"end\": END,             # Fertig\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "# Graph kompilieren\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Graph-Workflow testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Komplexere Anfrage, die mehrere Tools benötigen könnte\n",
    "result = graph.invoke({\n",
    "    \"input\": \"Ich möchte heute in Berlin spazieren gehen. Wie ist das Wetter dort? Außerdem will ich etwas Geld wechseln - wie viel bekomme ich für 150 EUR in Pfund?\",\n",
    "    \"intermediate_steps\": []\n",
    "})\n",
    "\n",
    "print(\"Finales Ergebnis:\")\n",
    "if \"answer\" in result:\n",
    "    print(result[\"answer\"])\n",
    "else:\n",
    "    print(\"Keine endgültige Antwort gefunden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Übungsaufgaben\n",
    "\n",
    "1. Erstellen Sie ein neues Tool, das mathematische Berechnungen durchführen kann (z.B. Addition, Subtraktion, Multiplikation, Division).\n",
    "2. Fügen Sie das Tool zu den bestehenden Tools hinzu und testen Sie es mit verschiedenen Anfragen.\n",
    "3. Erweitern Sie das Währungsumrechnungs-Tool um mindestens zwei weitere Währungen.\n",
    "4. Bonus: Implementieren Sie ein Tool, das einen kurzen Text zusammenfassen kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ihre Lösung für das Mathe-Tool\n",
    "@tool\n",
    "def mathe_berechnung(operation: str, zahl1: float, zahl2: float):\n",
    "    \"\"\"Führt eine mathematische Berechnung mit zwei Zahlen durch.\n",
    "    \n",
    "    Args:\n",
    "        operation: Die durchzuführende Operation (\"addition\", \"subtraktion\", \"multiplikation\", \"division\")\n",
    "        zahl1: Die erste Zahl\n",
    "        zahl2: Die zweite Zahl\n",
    "        \n",
    "    Returns:\n",
    "        Das Ergebnis der Berechnung\n",
    "    \"\"\"\n",
    "    # Ihre Implementierung hier\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Zusammenfassung\n",
    "\n",
    "In diesem Notebook haben wir gelernt:\n",
    "\n",
    "- Wie man eigene Tools mit dem `@tool`-Decorator erstellt\n",
    "- Wie man diese Tools an ein LLM bindet\n",
    "- Wie man mehrere Tools kombiniert für vielseitige Anfragen\n",
    "- Wie man Tools in einen LangGraph-Workflow integriert\n",
    "\n",
    "Tool Calls sind ein mächtiges Konzept, um die Fähigkeiten von LLMs zu erweitern und sie mit externen Funktionen, Daten und APIs zu verbinden. Sie ermöglichen dynamische, aktuelle und präzise Antworten auf komplexe Anfragen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-agents-Wmcp_RVb-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
