{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluationsmethoden f√ºr LLM-Anwendungen\n",
    "\n",
    "In diesem Notebook lernen Sie verschiedene Methoden zur Evaluation von LLM-Anwendungen kennen. Wir werden folgende Ans√§tze untersuchen:\n",
    "\n",
    "- LLM-As-A-Judge: Bewertung von LLM-Antworten durch andere LLMs\n",
    "- NLP-basierte Testkriterien (ROUGE, BLEU)\n",
    "- PI Scrubbing und Datenschutz mit Microsoft Presidio\n",
    "- RAG-spezifische Evaluierungsmethoden\n",
    "\n",
    "Diese Techniken helfen Ihnen, die Qualit√§t Ihrer KI-Anwendungen systematisch zu verbessern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation und Konfiguration der ben√∂tigten Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation der erforderlichen Pakete\n",
    "!pip install langchain langchain_openai langchain-core rouge-score nltk presidio-analyzer presidio-anonymizer -q\n",
    "\n",
    "# Umgebungsvariablen laden\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Grundlegende Importe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LangChain Importe\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation.criteria import LabeledCriteriaEvaluator\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisieren des LLMs\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# F√ºr Bewertungen verwenden wir ein separates LLM-Objekt\n",
    "evaluator_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLM-As-A-Judge: Grundlagen und Implementierung\n",
    "\n",
    "LLM-As-A-Judge ist ein Ansatz, bei dem ein LLM zur Bewertung der Ausgaben eines anderen LLMs verwendet wird. Dies ist besonders n√ºtzlich, wenn keine Referenzantworten verf√ºgbar sind oder die Bewertung menschlicher Pr√§ferenzen simuliert werden soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispielantworten f√ºr die Bewertung\n",
    "example_question = \"Was sind die wichtigsten Anwendungsf√§lle f√ºr Vektordatenbanken im Kontext von RAG-Systemen?\"\n",
    "\n",
    "response_a = \"\"\"Vektordatenbanken werden in RAG-Systemen haupts√§chlich f√ºr die effiziente Speicherung und den Abruf von Embeddings verwendet. \n",
    "Sie erm√∂glichen semantische Suche basierend auf √Ñhnlichkeit statt einfache Keyword-Suche. \n",
    "Weitere Anwendungsf√§lle sind Clusteranalyse und die Reduzierung der Dimensionalit√§t gro√üer Datens√§tze.\"\"\"\n",
    "\n",
    "response_b = \"\"\"Vektordatenbanken speichern Daten. Sie werden mit RAG verwendet, um Texte zu durchsuchen. \n",
    "Man kann darin Daten speichern und wieder abrufen, wenn man sie braucht. \n",
    "Sie sind besser als normale Datenbanken, weil sie schneller sind.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Evaluierung mit einem vordefinierten Evaluator\n",
    "qa_evaluator = load_evaluator(\"labeled_criteria\", criteria={\n",
    "    \"relevanz\": \"Ist die Antwort relevant f√ºr die Frage?\",\n",
    "    \"korrektheit\": \"Enth√§lt die Antwort korrekte und aktuelle Informationen?\",\n",
    "    \"vollst√§ndigkeit\": \"Beantwortet die Antwort die Frage vollst√§ndig?\",\n",
    "    \"klarheit\": \"Ist die Antwort klar, pr√§zise und gut strukturiert?\"\n",
    "}, llm=evaluator_llm)\n",
    "\n",
    "# Bewertung der Antworten\n",
    "eval_result_a = qa_evaluator.evaluate_strings(\n",
    "    prediction=response_a,\n",
    "    input=example_question\n",
    ")\n",
    "\n",
    "eval_result_b = qa_evaluator.evaluate_strings(\n",
    "    prediction=response_b,\n",
    "    input=example_question\n",
    ")\n",
    "\n",
    "print(\"Bewertung Antwort A:\")\n",
    "for criterion, rating in eval_result_a[\"criteria\"].items():\n",
    "    print(f\"{criterion}: {rating['rating']} - {rating['reasoning']}\")\n",
    "    \n",
    "print(\"\\nBewertung Antwort B:\")\n",
    "for criterion, rating in eval_result_b[\"criteria\"].items():\n",
    "    print(f\"{criterion}: {rating['rating']} - {rating['reasoning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenen LLM-As-A-Judge Evaluator implementieren\n",
    "\n",
    "LangChain bietet vordefinierte Evaluatoren, aber manchmal ben√∂tigen wir spezifischere Bewertungskriterien. Hier erstellen wir einen eigenen Evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenen Evaluator mit spezifischen Kriterien erstellen\n",
    "custom_criteria = {\n",
    "    \"technische_genauigkeit\": \"Enth√§lt die Antwort technisch korrekte Informationen √ºber Vektordatenbanken und RAG?\",\n",
    "    \"praxisbezug\": \"Gibt die Antwort praktische Beispiele oder Anwendungsf√§lle?\",\n",
    "    \"zielgruppenorientierung\": \"Ist die Antwort f√ºr Entwickler verst√§ndlich und hilfreich?\",\n",
    "    \"vollst√§ndigkeit\": \"Deckt die Antwort alle wichtigen Aspekte von Vektordatenbanken im RAG-Kontext ab?\"\n",
    "}\n",
    "\n",
    "custom_evaluator = LabeledCriteriaEvaluator(criteria=custom_criteria, llm=evaluator_llm)\n",
    "\n",
    "# Bewertung mit dem benutzerdefinierten Evaluator\n",
    "custom_eval_a = custom_evaluator.evaluate_strings(\n",
    "    prediction=response_a,\n",
    "    input=example_question\n",
    ")\n",
    "\n",
    "print(\"Benutzerdefinierte Bewertung f√ºr Antwort A:\")\n",
    "for criterion, rating in custom_eval_a[\"criteria\"].items():\n",
    "    print(f\"{criterion}: {rating['rating']} - {rating['reasoning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergleichende Bewertung\n",
    "\n",
    "Manchmal ist es sinnvoller, Antworten direkt miteinander zu vergleichen, anstatt sie einzeln zu bewerten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergleichenden Evaluator laden\n",
    "pairwise_evaluator = load_evaluator(\"pairwise_string\")\n",
    "\n",
    "# Direkte Gegen√ºberstellung der Antworten\n",
    "comparison = pairwise_evaluator.evaluate_string_pairs(\n",
    "    prediction_a=response_a,\n",
    "    prediction_b=response_b,\n",
    "    input=example_question\n",
    ")\n",
    "\n",
    "print(f\"Bevorzugte Antwort: {comparison['preferred']}\")\n",
    "print(f\"Begr√ºndung: {comparison['reasoning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NLP-basierte Testkriterien (ROUGE, BLEU, etc.)\n",
    "\n",
    "Neben LLM-basierten Evaluierungsmethoden k√∂nnen wir auch traditionelle NLP-Metriken verwenden, die auf Text√§hnlichkeit basieren. Diese sind besonders n√ºtzlich, wenn wir Referenzantworten haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE-Metrik importieren\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Referenzantwort (Gold-Standard)\n",
    "reference_answer = \"\"\"Vektordatenbanken spielen eine entscheidende Rolle in RAG-Systemen und bieten folgende Hauptanwendungsf√§lle:\n",
    "1. Effiziente Speicherung und Indexierung von Embeddings f√ºr schnellen Abruf\n",
    "2. Semantische Suche basierend auf Vektor√§hnlichkeit statt nur Schl√ºsselw√∂rtern\n",
    "3. Filterung und Abfrage mit Metadaten zus√§tzlich zur Vektorsuche\n",
    "4. Skalierbare Verwaltung gro√üer Dokumentenkorpora f√ºr RAG-Anwendungen\n",
    "5. Unterst√ºtzung f√ºr komplexe Abfragen mit Hybrid-Retrieval-Strategien\"\"\"\n",
    "\n",
    "# ROUGE-Scorer initialisieren\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# ROUGE-Scores berechnen\n",
    "rouge_scores_a = scorer.score(reference_answer, response_a)\n",
    "rouge_scores_b = scorer.score(reference_answer, response_b)\n",
    "\n",
    "print(\"ROUGE-Scores f√ºr Antwort A:\")\n",
    "for metric, score in rouge_scores_a.items():\n",
    "    print(f\"{metric}: Precision = {score.precision:.4f}, Recall = {score.recall:.4f}, F1 = {score.fmeasure:.4f}\")\n",
    "\n",
    "print(\"\\nROUGE-Scores f√ºr Antwort B:\")\n",
    "for metric, score in rouge_scores_b.items():\n",
    "    print(f\"{metric}: Precision = {score.precision:.4f}, Recall = {score.recall:.4f}, F1 = {score.fmeasure:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU-Score berechnen\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# NLTK-Daten herunterladen, falls noch nicht erfolgt\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Tokenisieren der Texte\n",
    "reference_tokens = nltk.word_tokenize(reference_answer.lower())\n",
    "response_a_tokens = nltk.word_tokenize(response_a.lower())\n",
    "response_b_tokens = nltk.word_tokenize(response_b.lower())\n",
    "\n",
    "# BLEU-Scores berechnen\n",
    "bleu_a = sentence_bleu([reference_tokens], response_a_tokens)\n",
    "bleu_b = sentence_bleu([reference_tokens], response_b_tokens)\n",
    "\n",
    "print(f\"BLEU-Score f√ºr Antwort A: {bleu_a:.4f}\")\n",
    "print(f\"BLEU-Score f√ºr Antwort B: {bleu_b:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung der Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnisse visualisieren\n",
    "metrics = ['ROUGE-1 F1', 'ROUGE-2 F1', 'ROUGE-L F1', 'BLEU']\n",
    "scores_a = [rouge_scores_a['rouge1'].fmeasure, rouge_scores_a['rouge2'].fmeasure, \n",
    "            rouge_scores_a['rougeL'].fmeasure, bleu_a]\n",
    "scores_b = [rouge_scores_b['rouge1'].fmeasure, rouge_scores_b['rouge2'].fmeasure, \n",
    "            rouge_scores_b['rougeL'].fmeasure, bleu_b]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "rects1 = ax.bar(x - width/2, scores_a, width, label='Antwort A')\n",
    "rects2 = ax.bar(x + width/2, scores_b, width, label='Antwort B')\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Vergleich der Evaluationsmetriken')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PI Scrubbing und Datenschutz mit Microsoft Presidio\n",
    "\n",
    "Bei der Evaluierung von LLM-Anwendungen ist Datenschutz ein wichtiger Aspekt. Microsoft Presidio ist ein Open-Source-Projekt f√ºr die Erkennung und Anonymisierung personenbezogener Daten (PII)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Presidio-Komponenten importieren\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "\n",
    "# Analyzer und Anonymizer initialisieren\n",
    "analyzer = AnalyzerEngine()\n",
    "anonymizer = AnonymizerEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispieltext mit personenbezogenen Daten\n",
    "text_mit_pii = \"\"\"\n",
    "Sehr geehrter Herr Schmidt,\n",
    "\n",
    "vielen Dank f√ºr Ihre Anfrage. Bitte kontaktieren Sie mich unter meiner E-Mail max.mustermann@example.com oder telefonisch unter +49 176 12345678.\n",
    "\n",
    "Ihr Kundenkonto mit der IBAN DE89 3704 0044 0532 0130 00 wurde aktualisiert.\n",
    "\n",
    "Mit freundlichen Gr√º√üen,\n",
    "Dr. Anna Weber\n",
    "\"\"\"\n",
    "\n",
    "# PII erkennen (deutsche Sprache)\n",
    "analyzer_results = analyzer.analyze(text=text_mit_pii, language=\"de\")\n",
    "\n",
    "print(\"Erkannte personenbezogene Daten:\")\n",
    "for result in analyzer_results:\n",
    "    print(f\"- {result.entity_type}: '{text_mit_pii[result.start:result.end]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text anonymisieren\n",
    "anonymized_text = anonymizer.anonymize(\n",
    "    text=text_mit_pii,\n",
    "    analyzer_results=analyzer_results\n",
    ")\n",
    "\n",
    "print(\"Anonymisierter Text:\")\n",
    "print(anonymized_text.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Anonymisieren von Evaluationsdaten\n",
    "def prepare_safe_evaluation_data(raw_text):\n",
    "    # PII erkennen\n",
    "    results = analyzer.analyze(text=raw_text, language=\"de\")\n",
    "    \n",
    "    # Wenn PII gefunden wurde, anonymisieren\n",
    "    if results:\n",
    "        anonymized = anonymizer.anonymize(\n",
    "            text=raw_text,\n",
    "            analyzer_results=results\n",
    "        )\n",
    "        return anonymized.text, True\n",
    "    else:\n",
    "        return raw_text, False\n",
    "\n",
    "# Beispiel f√ºr eine sichere Evaluierung\n",
    "evaluation_text = \"\"\"Herr Peter M√ºller (geboren am 15.04.1975) hat eine Anfrage zu seinem Konto 98765432 geschickt.\n",
    "Seine Telefonnummer lautet 030 12345678. Bitte evaluieren Sie die Qualit√§t der Antwort.\"\"\"\n",
    "\n",
    "safe_text, was_anonymized = prepare_safe_evaluation_data(evaluation_text)\n",
    "print(f\"Text wurde anonymisiert: {was_anonymized}\")\n",
    "print(f\"Sicherer Text f√ºr Evaluierung:\\n{safe_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAG-spezifische Evaluierungsmethoden\n",
    "\n",
    "F√ºr RAG-Systeme (Retrieval Augmented Generation) gibt es spezifische Evaluierungsmethoden, die sowohl die Retrieval-Komponente als auch die Generierungskomponente bewerten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel f√ºr RAG-Evaluierung\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Einfacher RAG-Prozess simulieren\n",
    "def simulate_rag(query, documents):\n",
    "    # Dokumente als Kontext zusammenf√ºgen\n",
    "    context = \"\\n\\n\".join([f\"Dokument {i+1}: {doc}\" for i, doc in enumerate(documents)])\n",
    "    \n",
    "    # Prompt f√ºr die Generierung\n",
    "    prompt_template = PromptTemplate.from_template(\n",
    "        \"\"\"Du bist ein hilfreicher Assistent. Verwende den folgenden Kontext, um die Frage zu beantworten.\n",
    "        \n",
    "        Kontext:\n",
    "        {context}\n",
    "        \n",
    "        Frage: {query}\n",
    "        \n",
    "        Deine Antwort:\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Chain erstellen und ausf√ºhren\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    result = chain.invoke({\"context\": context, \"query\": query})\n",
    "    \n",
    "    return result[\"text\"], context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel-Dokumente\n",
    "rag_documents = [\n",
    "    \"Vektordatenbanken sind spezielle Datenbanken, die f√ºr die Speicherung und Abfrage von Vektoren (Embeddings) optimiert sind. Sie erm√∂glichen schnelle √Ñhnlichkeitssuchen in hochdimensionalen R√§umen.\",\n",
    "    \"RAG (Retrieval Augmented Generation) ist ein Ansatz, bei dem ein LLM mit externen Informationen angereichert wird. Dadurch wird die Faktengenauigkeit verbessert und Halluzinationen werden reduziert.\",\n",
    "    \"Die Hauptvorteile von Vektordatenbanken in RAG-Systemen sind: schnelle √Ñhnlichkeitssuche, skalierbare Speicherung von Embeddings und die M√∂glichkeit zur Metadatenfilterung.\",\n",
    "    \"Chroma und Qdrant sind beliebte Vektordatenbanken f√ºr RAG-Anwendungen. Chroma ist einfach zu nutzen und ideal f√ºr Prototyping, w√§hrend Qdrant f√ºr Produktionsanwendungen mit hohen Anforderungen geeignet ist.\"\n",
    "]\n",
    "\n",
    "# RAG-Anfrage und Antwort generieren\n",
    "rag_query = \"Welche Vektordatenbanken sind f√ºr RAG-Systeme am besten geeignet und warum?\"\n",
    "rag_answer, rag_context = simulate_rag(rag_query, rag_documents)\n",
    "\n",
    "print(\"RAG-Antwort:\")\n",
    "print(rag_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG-spezifische Evaluierung\n",
    "\n",
    "# 1. Kontext-Relevanz pr√ºfen\n",
    "context_relevance_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"Du bist ein Experte f√ºr die Bewertung von Retrieval-Systemen. Deine Aufgabe ist es, die Relevanz der zur√ºckgegebenen Dokumente f√ºr eine Anfrage zu bewerten.\"),\n",
    "    HumanMessage(content=\"\"\"Bitte bewerte die Relevanz der folgenden Dokumente f√ºr die gegebene Anfrage. \n",
    "    Gib jedem Dokument eine Bewertung von 1-5 (1 = irrelevant, 5 = hochrelevant) und eine kurze Begr√ºndung.\n",
    "    \n",
    "    Anfrage: {query}\n",
    "    \n",
    "    Zur√ºckgegebene Dokumente:\n",
    "    {context}\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "context_eval_chain = LLMChain(llm=evaluator_llm, prompt=context_relevance_prompt)\n",
    "context_eval = context_eval_chain.invoke({\"query\": rag_query, \"context\": rag_context})\n",
    "\n",
    "print(\"Bewertung der Kontext-Relevanz:\")\n",
    "print(context_eval[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Treue zum Kontext (Faithfulness) pr√ºfen\n",
    "faithfulness_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"Du bist ein Experte f√ºr die Bewertung von KI-generierten Antworten. Deine Aufgabe ist es, zu pr√ºfen, ob eine Antwort durch den gegebenen Kontext unterst√ºtzt wird.\"),\n",
    "    HumanMessage(content=\"\"\"Bitte analysiere, ob die folgende Antwort vollst√§ndig durch den gegebenen Kontext unterst√ºtzt wird. \n",
    "    Identifiziere alle Behauptungen in der Antwort und bewerte, ob sie im Kontext enthalten sind oder Halluzinationen darstellen.\n",
    "    \n",
    "    Kontext:\n",
    "    {context}\n",
    "    \n",
    "    Antwort:\n",
    "    {answer}\n",
    "    \n",
    "    Gib eine Gesamtbewertung der Treue zum Kontext (1-5, wobei 5 die h√∂chste Treue bedeutet) und liste alle Halluzinationen oder nicht unterst√ºtzten Aussagen auf.\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "faithfulness_chain = LLMChain(llm=evaluator_llm, prompt=faithfulness_prompt)\n",
    "faithfulness_eval = faithfulness_chain.invoke({\"context\": rag_context, \"answer\": rag_answer})\n",
    "\n",
    "print(\"Bewertung der Treue zum Kontext:\")\n",
    "print(faithfulness_eval[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Antwortqualit√§t bewerten\n",
    "quality_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"Du bist ein Experte f√ºr die Bewertung von KI-generierten Antworten. Deine Aufgabe ist es, die Qualit√§t einer Antwort zu bewerten.\"),\n",
    "    HumanMessage(content=\"\"\"Bitte bewerte die Qualit√§t der folgenden Antwort auf die gegebene Frage nach diesen Kriterien:\n",
    "    1. Vollst√§ndigkeit (1-5): Beantwortet die Antwort alle Aspekte der Frage?\n",
    "    2. Korrektheit (1-5): Enth√§lt die Antwort korrekte Informationen?\n",
    "    3. Klarheit (1-5): Ist die Antwort klar und verst√§ndlich formuliert?\n",
    "    4. Pr√§gnanz (1-5): Ist die Antwort pr√§zise und auf den Punkt, ohne unn√∂tige Informationen?\n",
    "    \n",
    "    Frage: {query}\n",
    "    \n",
    "    Antwort:\n",
    "    {answer}\n",
    "    \n",
    "    Gib f√ºr jedes Kriterium eine Bewertung und eine kurze Begr√ºndung. Berechne auch einen Gesamtscore als Durchschnitt aller Kriterien.\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "quality_chain = LLMChain(llm=evaluator_llm, prompt=quality_prompt)\n",
    "quality_eval = quality_chain.invoke({\"query\": rag_query, \"answer\": rag_answer})\n",
    "\n",
    "print(\"Bewertung der Antwortqualit√§t:\")\n",
    "print(quality_eval[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ganzheitliche RAG-Evaluierung: RAGAS\n",
    "\n",
    "RAGAS ist ein Framework zur Evaluierung von RAG-Anwendungen, das verschiedene Aspekte eines RAG-Systems bewertet. Hier zeigen wir eine vereinfachte Version der RAGAS-Metriken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vereinfachte RAGAS-Evaluierung\n",
    "ragas_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"Du bist ein Experte f√ºr die Evaluierung von RAG-Systemen (Retrieval Augmented Generation). Deine Aufgabe ist es, ein RAG-System nach den RAGAS-Metriken zu bewerten.\"),\n",
    "    HumanMessage(content=\"\"\"Bitte evaluiere das folgende RAG-System nach diesen RAGAS-Metriken:\n",
    "    \n",
    "    1. Kontext-Relevanz: Sind die abgerufenen Dokumente relevant f√ºr die Anfrage?\n",
    "    2. Treue (Faithfulness): Wird die Antwort durch den Kontext unterst√ºtzt?\n",
    "    3. Antwort-Relevanz: Beantwortet die generierte Antwort die urspr√ºngliche Frage?\n",
    "    4. Kontext-Nutzung: Wie gut nutzt die Antwort den bereitgestellten Kontext?\n",
    "    \n",
    "    Anfrage: {query}\n",
    "    \n",
    "    Abgerufene Dokumente:\n",
    "    {context}\n",
    "    \n",
    "    Generierte Antwort:\n",
    "    {answer}\n",
    "    \n",
    "    Gib f√ºr jede Metrik eine Bewertung von 0-1 (0 = schlecht, 1 = hervorragend) und berechne einen Gesamtscore als gewichteten Durchschnitt.\n",
    "    Gib au√üerdem konkrete Verbesserungsvorschl√§ge f√ºr das RAG-System.\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "ragas_chain = LLMChain(llm=evaluator_llm, prompt=ragas_prompt)\n",
    "ragas_eval = ragas_chain.invoke({\"query\": rag_query, \"context\": rag_context, \"answer\": rag_answer})\n",
    "\n",
    "print(\"RAGAS-Evaluierung:\")\n",
    "print(ragas_eval[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Systematische Evaluierung mit einem Testdatensatz\n",
    "\n",
    "F√ºr eine gr√ºndliche Evaluierung sollten wir mehrere Anfragen testen und die Ergebnisse aggregieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testdatensatz mit Fragen und erwarteten Antworten\n",
    "test_dataset = [\n",
    "    {\n",
    "        \"query\": \"Welche Vektordatenbanken sind f√ºr RAG-Systeme am besten geeignet?\",\n",
    "        \"expected_answer\": \"Chroma eignet sich gut f√ºr Prototyping, w√§hrend Qdrant f√ºr Produktionsanwendungen empfohlen wird.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Was ist der Hauptvorteil von Vektordatenbanken in RAG?\",\n",
    "        \"expected_answer\": \"Schnelle √Ñhnlichkeitssuche und skalierbare Speicherung von Embeddings.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Erkl√§re den Unterschied zwischen RAG und reinen LLM-Anwendungen.\",\n",
    "        \"expected_answer\": \"RAG erg√§nzt LLMs mit externen Informationen zur Verbesserung der Faktengenauigkeit.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Funktion zur Durchf√ºhrung einer systematischen Evaluierung\n",
    "def evaluate_rag_system(test_cases, documents):\n",
    "    results = []\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        query = test_case[\"query\"]\n",
    "        expected = test_case[\"expected_answer\"]\n",
    "        \n",
    "        # RAG-Antwort generieren\n",
    "        answer, context = simulate_rag(query, documents)\n",
    "        \n",
    "        # Grundlegende Metriken berechnen\n",
    "        # 1. Rouge-Score f√ºr √Ñhnlichkeit zur erwarteten Antwort\n",
    "        rouge_scores = scorer.score(expected, answer)\n",
    "        rouge_f1 = rouge_scores[\"rougeL\"].fmeasure\n",
    "        \n",
    "        # 2. LLM-basierte Bewertung\n",
    "        eval_prompt = f\"\"\"Bewerte die folgende Antwort auf die Frage: \\\"{query}\\\" \n",
    "        auf einer Skala von 1-10 f√ºr Genauigkeit, Vollst√§ndigkeit und Klarheit. \n",
    "        Erwartete Antwort: {expected}\n",
    "        Tats√§chliche Antwort: {answer}\"\"\"\n",
    "        \n",
    "        evaluation = evaluator_llm.invoke(eval_prompt)\n",
    "        \n",
    "        # Ergebnisse sammeln\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"expected\": expected,\n",
    "            \"answer\": answer,\n",
    "            \"rouge_f1\": rouge_f1,\n",
    "            \"llm_evaluation\": evaluation.content\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluierung durchf√ºhren\n",
    "evaluation_results = evaluate_rag_system(test_dataset, rag_documents)\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "for i, result in enumerate(evaluation_results):\n",
    "    print(f\"\\nTest {i+1}: {result['query']}\")\n",
    "    print(f\"Rouge-L F1: {result['rouge_f1']:.4f}\")\n",
    "    print(f\"LLM-Bewertung: {result['llm_evaluation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Zusammenfassung: Evaluierungsworkflow\n",
    "\n",
    "Basierend auf den vorgestellten Methoden k√∂nnen wir einen umfassenden Evaluierungsworkflow f√ºr LLM- und RAG-Anwendungen erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Umfassender Evaluierungsworkflow f√ºr eine LLM-Anwendung\n",
    "def comprehensive_evaluation_workflow(query, documents, reference_answer=None):\n",
    "    print(f\"Evaluierung f√ºr Anfrage: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 1. Datenschutz-Check und Anonymisierung\n",
    "    safe_query, was_anonymized = prepare_safe_evaluation_data(query)\n",
    "    if was_anonymized:\n",
    "        print(\"‚ö†Ô∏è Anfrage enthielt personenbezogene Daten und wurde anonymisiert.\")\n",
    "        print(f\"Anonymisierte Anfrage: {safe_query}\")\n",
    "    \n",
    "    # 2. RAG-Antwort generieren\n",
    "    answer, context = simulate_rag(safe_query, documents)\n",
    "    print(\"\\nüìù Generierte Antwort:\")\n",
    "    print(answer)\n",
    "    \n",
    "    # 3. Metriken berechnen\n",
    "    metrics = {}\n",
    "    \n",
    "    # 3.1 Wenn eine Referenzantwort vorhanden ist, klassische NLP-Metriken verwenden\n",
    "    if reference_answer:\n",
    "        rouge_scores = scorer.score(reference_answer, answer)\n",
    "        metrics[\"ROUGE-L F1\"] = rouge_scores[\"rougeL\"].fmeasure\n",
    "        \n",
    "        reference_tokens = nltk.word_tokenize(reference_answer.lower())\n",
    "        answer_tokens = nltk.word_tokenize(answer.lower())\n",
    "        metrics[\"BLEU\"] = sentence_bleu([reference_tokens], answer_tokens)\n",
    "    \n",
    "    # 3.2 LLM-basierte Evaluierung f√ºr Qualit√§t\n",
    "    quality_eval = quality_chain.invoke({\"query\": safe_query, \"answer\": answer})\n",
    "    \n",
    "    # 3.3 RAG-spezifische Evaluierung\n",
    "    faith_eval = faithfulness_chain.invoke({\"context\": context, \"answer\": answer})\n",
    "    context_eval = context_eval_chain.invoke({\"query\": safe_query, \"context\": context})\n",
    "    \n",
    "    # 4. Ergebnisse zusammenfassen\n",
    "    print(\"\\nüìä Evaluierungsergebnisse:\")\n",
    "    if reference_answer:\n",
    "        print(f\"ROUGE-L F1: {metrics['ROUGE-L F1']:.4f}\")\n",
    "        print(f\"BLEU Score: {metrics['BLEU']:.4f}\")\n",
    "    \n",
    "    print(\"\\nüîç Qualit√§tsbewertung:\")\n",
    "    print(quality_eval[\"text\"])\n",
    "    \n",
    "    print(\"\\n‚úì Treue zum Kontext:\")\n",
    "    print(faith_eval[\"text\"])\n",
    "    \n",
    "    print(\"\\nüìö Relevanz der abgerufenen Dokumente:\")\n",
    "    print(context_eval[\"text\"])\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"context\": context,\n",
    "        \"metrics\": metrics,\n",
    "        \"quality_evaluation\": quality_eval[\"text\"],\n",
    "        \"faithfulness_evaluation\": faith_eval[\"text\"],\n",
    "        \"context_evaluation\": context_eval[\"text\"],\n",
    "    }\n",
    "\n",
    "# Beispiel f√ºr den umfassenden Evaluierungsworkflow\n",
    "reference = \"F√ºr RAG-Systeme sind Chroma und Qdrant gut geeignete Vektordatenbanken. Chroma ist einfach zu nutzen und ideal f√ºr Prototyping, w√§hrend Qdrant f√ºr Produktionsanwendungen mit hohen Anforderungen geeignet ist.\"\n",
    "eval_results = comprehensive_evaluation_workflow(rag_query, rag_documents, reference_answer=reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. √úbungen\n",
    "\n",
    "Hier sind einige √úbungen, um Ihr Wissen √ºber Evaluationsmethoden zu vertiefen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √úbung 1: Eigenen Evaluator implementieren\n",
    "\n",
    "Erstellen Sie einen benutzerdefinierten Evaluator f√ºr einen spezifischen Anwendungsfall (z.B. technische Dokumentation, medizinische Beratung, oder Kundensupport).\n",
    "\n",
    "1. Definieren Sie 3-5 dom√§nenspezifische Kriterien f√ºr Ihre Bewertung\n",
    "2. Implementieren Sie diese mit dem `LabeledCriteriaEvaluator`\n",
    "3. Testen Sie Ihren Evaluator mit mindestens 2 verschiedenen Antworten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √úbung 2: Vergleichende RAG-Evaluation\n",
    "\n",
    "Vergleichen Sie zwei verschiedene RAG-Setups und evaluieren Sie ihre Leistung:\n",
    "\n",
    "1. Setup A: Standard-RAG mit allen Dokumenten\n",
    "2. Setup B: RAG mit Vorfilterung (z.B. nur die 2 relevantesten Dokumente verwenden)\n",
    "\n",
    "Verwenden Sie die gleichen 3 Testanfragen f√ºr beide Setups und analysieren Sie die Unterschiede in den Ergebnissen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √úbung 3: Dashboard f√ºr Evaluierungsergebnisse\n",
    "\n",
    "Erstellen Sie ein einfaches Dashboard zur Visualisierung von Evaluierungsergebnissen:\n",
    "\n",
    "1. F√ºhren Sie mindestens 5 Tests mit verschiedenen Anfragen durch\n",
    "2. Speichern Sie die Ergebnisse in einem DataFrame\n",
    "3. Erstellen Sie Visualisierungen f√ºr verschiedene Metriken\n",
    "4. Berechnen Sie Durchschnittswerte und identifizieren Sie Verbesserungspotenzial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Schlussfolgerungen und Best Practices\n",
    "\n",
    "- **Kombinieren Sie mehrere Evaluierungsmethoden**: Nutzen Sie sowohl automatische Metriken als auch LLM-basierte Bewertungen f√ºr ein umfassenderes Bild.\n",
    "\n",
    "- **Datenschutz beachten**: Stellen Sie sicher, dass Ihre Evaluierungsdaten keine personenbezogenen Informationen enthalten oder diese angemessen anonymisiert sind.\n",
    "\n",
    "- **Kontinuierlich evaluieren**: Bauen Sie Evaluierung in Ihren Entwicklungsprozess ein, nicht nur als einmalige Aktivit√§t am Ende.\n",
    "\n",
    "- **Repr√§sentative Testdaten**: Stellen Sie sicher, dass Ihre Testf√§lle die tats√§chlichen Nutzungsszenarien repr√§sentieren.\n",
    "\n",
    "- **Menschliche √úberpr√ºfung**: Automatische Evaluierung sollte durch menschliche √úberpr√ºfung erg√§nzt werden, besonders bei wichtigen Anwendungsf√§llen.\n",
    "\n",
    "- **Ergebnisse dokumentieren**: Halten Sie Ihre Evaluierungsergebnisse fest, um Verbesserungen im Laufe der Zeit zu verfolgen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
