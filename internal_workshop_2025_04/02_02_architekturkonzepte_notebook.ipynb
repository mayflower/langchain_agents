{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architekturkonzepte in LLM-Anwendungen\n",
    "\n",
    "In diesem Notebook werden wir verschiedene Architekturkonzepte für LLM-Anwendungen untersuchen:\n",
    "\n",
    "1. Agentenbasierte Ansätze: Grundlagen und Vorteile\n",
    "2. ReAct Pattern: Reasoning and Acting in LLMs\n",
    "3. Graph-RAG und Hierarchical RAG: Fortgeschrittene RAG-Architekturen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benötigte Bibliotheken installieren\n",
    "\n",
    "Zunächst müssen wir sicherstellen, dass alle notwendigen Bibliotheken installiert sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install langchain langchain-community langchain-openai langgraph python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Laden der Umgebungsvariablen aus der .env-Datei\n",
    "load_dotenv()\n",
    "\n",
    "# Überprüfen, ob der OpenAI API-Schlüssel gesetzt ist\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"OpenAI API-Schlüssel ist konfiguriert.\")\n",
    "else:\n",
    "    print(\"WARNUNG: OpenAI API-Schlüssel fehlt! Bitte in der .env-Datei konfigurieren.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Agentenbasierte Ansätze\n",
    "\n",
    "### Grundlagen von Agenten\n",
    "\n",
    "Agenten sind KI-Systeme, die selbstständig Entscheidungen treffen und Aktionen ausführen können. Sie kombinieren LLMs mit Tools und einem Entscheidungsprozess, um komplexe Aufgaben zu lösen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "\n",
    "# Einfachen LLM initialisieren\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Einige einfache Tools definieren\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "\n",
    "# Eigenes Tool erstellen\n",
    "@tool\n",
    "def aktuelle_zeit(timezone: str = \"Europe/Berlin\") -> str:\n",
    "    \"\"\"Gibt die aktuelle Uhrzeit in der angegebenen Zeitzone zurück.\"\"\"\n",
    "    from datetime import datetime\n",
    "    from zoneinfo import ZoneInfo\n",
    "    try:\n",
    "        return datetime.now(ZoneInfo(timezone)).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "    except Exception as e:\n",
    "        return f\"Fehler: {str(e)}\"\n",
    "\n",
    "# Tool zur Tool-Liste hinzufügen\n",
    "tools.append(aktuelle_zeit)\n",
    "\n",
    "# Einfachen Agenten erstellen\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Agenten ausführen\n",
    "agent.invoke(\"Wie lautet die aktuelle Uhrzeit in Berlin und berechne dann 15 hoch 0.5?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorteile agentenbasierter Ansätze\n",
    "\n",
    "- **Autonomie**: Agenten können eigenständig Entscheidungen treffen und Aktionen ausführen\n",
    "- **Flexibilität**: Können mit vielen verschiedenen Tools arbeiten\n",
    "- **Komplexität**: Können mehrstufige Probleme in Teilschritte zerlegen\n",
    "- **Erweiterbarkeit**: Leichte Integration neuer Tools und Funktionen\n",
    "\n",
    "![Agent Architektur](https://upload.wikimedia.org/wikipedia/commons/2/21/Agent_based_modelling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ReAct Pattern: Reasoning and Acting in LLMs\n",
    "\n",
    "Das ReAct-Muster kombiniert Reasoning (Denken) und Acting (Handeln) in einem iterativen Prozess.\n",
    "\n",
    "### Komponenten des ReAct-Musters:\n",
    "\n",
    "1. **Thought (Gedanke)**: Das LLM überlegt, wie es vorgehen soll\n",
    "2. **Action (Aktion)**: Das LLM wählt eine Aktion/Tool aus und führt sie aus\n",
    "3. **Observation (Beobachtung)**: Das LLM erhält die Ergebnisse der Aktion\n",
    "4. **Wiederholen**: Bis die Aufgabe gelöst ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Implementierung des ReAct-Musters mit LangChain\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# LLM initialisieren\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Tools laden (wir nutzen serpapi für Web-Suche und llm-math für Berechnungen)\n",
    "# Hinweis: Sie benötigen einen SERPAPI-Schlüssel in der .env-Datei\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# ReAct-Agent initialisieren\n",
    "react_agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # Dies ist der ReAct-Agent\n",
    "    verbose=True  # Ausführliche Ausgabe, um den Denkprozess zu sehen\n",
    ")\n",
    "\n",
    "# Komplexe Frage stellen, die mehrere Schritte erfordert\n",
    "react_agent.invoke(\n",
    "    \"Wie alt ist Angela Merkel und was ist diese Zahl quadriert?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktionsweise des ReAct-Musters\n",
    "\n",
    "1. Das LLM wird mit einer **promt template** angewiesen, seine Gedanken zu verbalisieren\n",
    "2. Der Agent formuliert einen **Gedanken** darüber, wie er die Aufgabe angehen sollte\n",
    "3. Der Agent wählt eine **Aktion** und ein Tool aus der verfügbaren Tool-Liste\n",
    "4. Das Tool wird ausgeführt und liefert eine **Beobachtung** zurück\n",
    "5. Der Agent überlegt, basierend auf der Beobachtung, was als nächstes zu tun ist\n",
    "6. Dieser Prozess wiederholt sich, bis der Agent glaubt, die Aufgabe gelöst zu haben\n",
    "\n",
    "### Vorteile des ReAct-Musters:\n",
    "\n",
    "- **Transparenz**: Der Denkprozess des Agenten ist sichtbar\n",
    "- **Bessere Entscheidungen**: Durch explizites Nachdenken werden bessere Entscheidungen getroffen\n",
    "- **Selbstkorrektur**: Der Agent kann Fehler erkennen und korrigieren\n",
    "- **Systematisches Vorgehen**: Komplexe Probleme werden strukturiert gelöst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph-RAG und Hierarchical RAG\n",
    "\n",
    "RAG (Retrieval Augmented Generation) verbessert LLM-Antworten durch die Einbindung externer Informationen. Graph-RAG und Hierarchical RAG sind fortgeschrittene Architekturen, die die Leistung von RAG-Systemen verbessern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph-RAG\n",
    "\n",
    "Graph-RAG nutzt Graphstrukturen, um Beziehungen zwischen Dokumenten und Konzepten zu modellieren.\n",
    "\n",
    "**Schlüsselkonzepte:**\n",
    "- Dokumente und Informationen werden als Knoten in einem Graphen dargestellt\n",
    "- Beziehungen zwischen Informationen werden als Kanten modelliert\n",
    "- Der Graph ermöglicht kontextreicheres Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Einfaches Beispiel für Graph-RAG mit LangGraph\n",
    "from typing import Annotated, TypedDict, List, Dict, Any\n",
    "from langgraph.graph import StateGraph\n",
    "import operator\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Definieren eines einfachen Zustands für unseren Graphen\n",
    "class GraphState(TypedDict):\n",
    "    query: str\n",
    "    context: List[str]\n",
    "    answer: str\n",
    "\n",
    "# Embeddings initialisieren\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# LLM initialisieren\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Beispieldaten für die Vektordatenbank\n",
    "sample_texts = [\n",
    "    \"Berlin ist die Hauptstadt von Deutschland.\",\n",
    "    \"München ist die Hauptstadt von Bayern.\",\n",
    "    \"Hamburg ist die zweitgrößte Stadt Deutschlands.\",\n",
    "    \"Frankfurt ist ein wichtiges Finanzzentrum in Europa.\",\n",
    "    \"Köln ist bekannt für seinen Dom.\"\n",
    "]\n",
    "\n",
    "# Vektordatenbank erstellen\n",
    "vectorstore = Chroma.from_texts(sample_texts, embeddings)\n",
    "\n",
    "# Knoten-Funktionen definieren\n",
    "def retrieve(state: GraphState) -> GraphState:\n",
    "    \"\"\"Dokumente aus der Vektordatenbank abrufen\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    # Top 2 relevante Dokumente abrufen\n",
    "    docs = vectorstore.similarity_search(query, k=2)\n",
    "    return {\"context\": [doc.page_content for doc in docs]}\n",
    "\n",
    "def generate_answer(state: GraphState) -> GraphState:\n",
    "    \"\"\"Antwort basierend auf dem Kontext generieren\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    context = state[\"context\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Du bist ein hilfreicher Assistent. \n",
    "        Verwende den folgenden Kontext, um die Frage zu beantworten.\n",
    "        \n",
    "        Kontext: {context}\n",
    "        \n",
    "        Frage: {query}\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    answer = chain.invoke({\"context\": \"\\n\".join(context), \"query\": query})\n",
    "    \n",
    "    return {\"answer\": answer}\n",
    "\n",
    "# Graph erstellen\n",
    "graph = StateGraph(GraphState)\n",
    "\n",
    "# Knoten hinzufügen\n",
    "graph.add_node(\"retrieve\", retrieve)\n",
    "graph.add_node(\"generate\", generate_answer)\n",
    "\n",
    "# Kanten definieren\n",
    "graph.set_entry_point(\"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"generate\")\n",
    "graph.set_finish_point(\"generate\")\n",
    "\n",
    "# Graph kompilieren\n",
    "chain = graph.compile()\n",
    "\n",
    "# Graph testen\n",
    "result = chain.invoke({\"query\": \"Was ist die Hauptstadt von Deutschland?\", \"context\": [], \"answer\": \"\"})\n",
    "print(\"Antwort:\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical RAG\n",
    "\n",
    "Hierarchical RAG organisiert Informationen in Hierarchien, um den Suchraum effizient einzugrenzen.\n",
    "\n",
    "**Schlüsselkonzepte:**\n",
    "- Informationen werden in verschiedenen Ebenen organisiert\n",
    "- Die Suche beginnt auf einer hohen Ebene und verfeinert sich schrittweise\n",
    "- Ermöglicht effizientere Informationsextraktion bei großen Dokumentenmengen\n",
    "\n",
    "#### Beispielarchitektur für Hierarchical RAG:\n",
    "\n",
    "1. **Oberste Ebene**: Dokumenttitel und Zusammenfassungen\n",
    "2. **Mittlere Ebene**: Abschnitte und Kapitel\n",
    "3. **Unterste Ebene**: Detaillierte Textpassagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Konzeptionelles Beispiel für Hierarchical RAG (vereinfacht)\n",
    "from typing import List, Dict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM initialisieren\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Vereinfachte Dokument-Hierarchie\n",
    "document_hierarchy = {\n",
    "    \"level1\": [\n",
    "        {\"id\": \"doc1\", \"summary\": \"Deutschlands Großstädte und ihre Bedeutung\"},\n",
    "        {\"id\": \"doc2\", \"summary\": \"Europäische Hauptstädte im Vergleich\"}\n",
    "    ],\n",
    "    \"level2\": {\n",
    "        \"doc1\": [\n",
    "            {\"id\": \"doc1_section1\", \"title\": \"Berlin als Hauptstadt\"},\n",
    "            {\"id\": \"doc1_section2\", \"title\": \"Hamburg als Handelsmetropole\"}\n",
    "        ],\n",
    "        \"doc2\": [\n",
    "            {\"id\": \"doc2_section1\", \"title\": \"Berlin im europäischen Kontext\"},\n",
    "            {\"id\": \"doc2_section2\", \"title\": \"Paris als Kulturzentrum\"}\n",
    "        ]\n",
    "    },\n",
    "    \"level3\": {\n",
    "        \"doc1_section1\": \"Berlin ist die Hauptstadt und bevölkerungsreichste Stadt Deutschlands. Mit rund 3,7 Millionen Einwohnern ist Berlin auch die größte Stadt der Europäischen Union.\",\n",
    "        \"doc1_section2\": \"Hamburg ist mit 1,8 Millionen Einwohnern die zweitgrößte Stadt Deutschlands und ein bedeutendes Wirtschafts- und Handelszentrum in Nordeuropa.\",\n",
    "        \"doc2_section1\": \"Im Vergleich zu anderen europäischen Hauptstädten hat Berlin eine besondere Geschichte aufgrund der deutschen Teilung im 20. Jahrhundert.\",\n",
    "        \"doc2_section2\": \"Paris, die Hauptstadt Frankreichs, gilt als eines der wichtigsten Kulturzentren Europas mit berühmten Museen wie dem Louvre und dem Centre Pompidou.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def hierarchical_search(query: str) -> str:\n",
    "    \"\"\"Führt eine hierarchische Suche durch\"\"\"\n",
    "    \n",
    "    # 1. Schritt: Auswahl relevanter Dokumente auf Level 1\n",
    "    level1_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Gegeben sind die folgenden Dokumentzusammenfassungen:\n",
    "        {summaries}\n",
    "        \n",
    "        Für die Anfrage: {query}\n",
    "        Gib die ID des relevantesten Dokuments zurück. Antworte nur mit der ID.\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Zusammenfassungen zusammenstellen\n",
    "    summaries = \"\\n\".join([f\"ID: {doc['id']}, Zusammenfassung: {doc['summary']}\" \n",
    "                           for doc in document_hierarchy[\"level1\"]])\n",
    "    \n",
    "    # Level 1 Auswahl treffen\n",
    "    level1_chain = level1_prompt | llm | StrOutputParser()\n",
    "    selected_doc = level1_chain.invoke({\"summaries\": summaries, \"query\": query})\n",
    "    print(f\"Ausgewähltes Dokument Level 1: {selected_doc}\")\n",
    "    \n",
    "    # 2. Schritt: Auswahl relevanter Abschnitte auf Level 2\n",
    "    level2_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Gegeben sind die folgenden Abschnitte aus dem Dokument {doc_id}:\n",
    "        {sections}\n",
    "        \n",
    "        Für die Anfrage: {query}\n",
    "        Gib die ID des relevantesten Abschnitts zurück. Antworte nur mit der ID.\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Abschnitte zusammenstellen\n",
    "    sections = \"\\n\".join([f\"ID: {section['id']}, Titel: {section['title']}\" \n",
    "                          for section in document_hierarchy[\"level2\"].get(selected_doc.strip(), [])])\n",
    "    \n",
    "    # Level 2 Auswahl treffen\n",
    "    level2_chain = level2_prompt | llm | StrOutputParser()\n",
    "    selected_section = level2_chain.invoke({\"doc_id\": selected_doc, \"sections\": sections, \"query\": query})\n",
    "    print(f\"Ausgewählter Abschnitt Level 2: {selected_section}\")\n",
    "    \n",
    "    # 3. Schritt: Detailtext abrufen und Antwort generieren\n",
    "    detail_text = document_hierarchy[\"level3\"].get(selected_section.strip(), \"Keine Informationen gefunden.\")\n",
    "    \n",
    "    answer_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Basierend auf dem folgenden Text, beantworte die Frage.\n",
    "        \n",
    "        Text: {text}\n",
    "        \n",
    "        Frage: {query}\n",
    "        \n",
    "        Antwort:\"\"\"\n",
    "    )\n",
    "    \n",
    "    answer_chain = answer_prompt | llm | StrOutputParser()\n",
    "    answer = answer_chain.invoke({\"text\": detail_text, \"query\": query})\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Testen der hierarchischen Suche\n",
    "result = hierarchical_search(\"Wie viele Einwohner hat Berlin?\")\n",
    "print(\"\\nAntwort:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In diesem Notebook haben wir drei wichtige Architekturkonzepte für LLM-Anwendungen kennengelernt:\n",
    "\n",
    "1. **Agentenbasierte Ansätze**:\n",
    "   - Agenten kombinieren LLMs mit Tools für komplexe Aufgaben\n",
    "   - Sie bieten Autonomie, Flexibilität und einfache Erweiterbarkeit\n",
    "\n",
    "2. **ReAct Pattern**:\n",
    "   - Kombiniert Reasoning und Acting in einem iterativen Prozess\n",
    "   - Verbessert die Transparenz und Qualität von Entscheidungen\n",
    "   - Ermöglicht systematisches Problemlösen\n",
    "\n",
    "3. **Graph-RAG und Hierarchical RAG**:\n",
    "   - Graph-RAG nutzt Beziehungen zwischen Informationen\n",
    "   - Hierarchical RAG organisiert Wissen in verschiedenen Abstraktionsebenen\n",
    "   - Beide verbessern die Effektivität von RAG-Systemen\n",
    "\n",
    "Diese Konzepte können kombiniert werden, um leistungsfähige KI-Systeme zu entwickeln, die komplexe Aufgaben effizient lösen können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Übungen\n",
    "\n",
    "1. Erweitern Sie den ReAct-Agenten um ein eigenes Tool (z.B. Wetter-API oder Währungsumrechnung)\n",
    "2. Modifizieren Sie den Graph-RAG-Ansatz, um einen Feedback-Knoten einzubauen, der die Antwort verbessert\n",
    "3. Entwerfen Sie eine hierarchische RAG-Struktur für ein spezielles Fachgebiet (z.B. Medizin oder Rechtswesen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}